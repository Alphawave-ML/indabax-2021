{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564f41e-fb34-4e8b-90c9-291f61d21543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.557719Z",
     "start_time": "2021-09-29T07:11:27.823599Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import gym\n",
    "import pandas as pd\n",
    "import gym_minigrid\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "normalize_qs = Normalize(vmin = -1, vmax = 1)\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    margin:auto;\n",
    "}\n",
    ".prompt \n",
    "    display:none;\n",
    "} \n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bfd55",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def render(env, mode='rgb'):\n",
    "    img = env.render(mode=mode, highlight=False)\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    clear_output(wait=True)\n",
    "    plt.pause(0.00001)\n",
    "\n",
    "\n",
    "def draw_triangle(ax, p1=[1, 1], p2=[2, 2.5], p3=[3, 1], c='blue', alpha=0.3):\n",
    "    X = np.array([p1, p2, p3])\n",
    "    Y = [c, c, c]\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=0.001, color=Y[:])\n",
    "    t1 = plt.Polygon(X[:3, :], color=Y[0], alpha=alpha)\n",
    "    ax.add_patch(t1)\n",
    "\n",
    "\n",
    "def draw_square(ax,\n",
    "                lb_x,\n",
    "                lb_y,\n",
    "                size,\n",
    "                alpha=1,\n",
    "                l_color='green',  # left\n",
    "                r_color='yellow',  # right\n",
    "                b_color='red',  # down\n",
    "                t_color='blue'):  # up\n",
    "    tr_x = lb_x+size  # top right x\n",
    "    tr_y = lb_y+size  # top right y\n",
    "\n",
    "    md_x = lb_x + size/2  # middle x\n",
    "    md_y = lb_y + size/2  # middle x\n",
    "\n",
    "    width = 10\n",
    "\n",
    "    md_x_plus = md_x + width/2\n",
    "    md_x_minus = md_x - width/2\n",
    "\n",
    "    md_y_plus = md_y + width/2\n",
    "    md_y_minus = md_y - width/2\n",
    "\n",
    "    colormap = cm.get_cmap('RdYlGn')\n",
    "\n",
    "    color_right = colormap(r_color)\n",
    "    color_down = colormap(b_color)\n",
    "    color_left = colormap(l_color)\n",
    "    color_up = colormap(t_color)\n",
    "\n",
    "    draw_triangle(ax,[lb_x+2, md_y], [md_x_minus, md_y_minus+2],\n",
    "                  [md_x_minus, md_y_plus-2], c=color_left, alpha=alpha)  # left\n",
    "    draw_triangle(ax,[tr_x-2, md_y], [md_x_plus, md_y_minus+2],\n",
    "                  [md_x_plus, md_y_plus-2], c=color_right, alpha=alpha)  # right\n",
    "    draw_triangle(ax,[md_x, tr_y-2], [md_x_minus+2, md_y_plus],\n",
    "                  [md_x_plus-2, md_y_plus], c=color_down, alpha=alpha)  # down\n",
    "    draw_triangle(ax,[md_x, lb_y+2], [md_x_minus+2, md_y_minus],\n",
    "                  [md_x_plus-2, md_y_minus], c=color_up, alpha=alpha)  # up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65df484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_return(ax, returns, y_min, y_max):\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Return')\n",
    "    ax.set_title('Return versus Episodes')\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.plot(np.array(returns), linestyle='dashed', color='silver', lw=1.5)\n",
    "    ax.plot(pd.Series(returns).rolling(window=10).mean().values, lw=2)\n",
    "\n",
    "def plot_epsilon(ax, epsilon_history):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Epsilon')\n",
    "    ax.set_title('Epsilon versus Episodes')\n",
    "    ax.plot(epsilon_history)\n",
    "\n",
    "def plot_q_value_func(ax, table, env, alpha = 1, img = None):\n",
    "    df_table = pd.DataFrame(table).T.reset_index()\n",
    "    df_table.columns = ['y', 'x', 'right', 'down', 'left', 'up']\n",
    "    df_table = df_table.sort_values(by=['y','x'])\n",
    "    df_table['lb_x'] = df_table['x']*32\n",
    "    df_table['lb_y'] = df_table['y']*32\n",
    "\n",
    "    df_table[['right', 'down', 'left', 'up']] = normalize_qs(df_table[['right', 'down', 'left', 'up']])\n",
    "    if img is None:\n",
    "        img = env.grid.render(tile_size=32)\n",
    "    ax.imshow(img, interpolation='nearest')\n",
    "    for row in df_table.itertuples():             \n",
    "        draw_square(ax,\n",
    "                    row[-2], # lb_x\n",
    "                    row[-1], # lb_y\n",
    "                    32, \n",
    "                    alpha=alpha, \n",
    "                    r_color=row[3], # right\n",
    "                    b_color=row[4], # down\n",
    "                    l_color=row[5], # left\n",
    "                    t_color=row[6]) # up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489751d",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "from gym.spaces import Box\n",
    "from gym_minigrid.minigrid import Goal\n",
    "\n",
    "\n",
    "class DirectionObsWrapper(gym.core.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Provides the slope/angular direction to the goal with the observations as modeled by (y2 - y2 )/( x2 - x1)\n",
    "    type = {slope , angle}\n",
    "    \"\"\"\n",
    "    def __init__(self, env,type='angle'):\n",
    "        super().__init__(env)\n",
    "        self.goal_position = None\n",
    "        self.type = type\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if not self.goal_position:\n",
    "            self.goal_position = [x for x,y in enumerate(self.grid.grid) if isinstance(y,(Goal) ) ]\n",
    "            if len(self.goal_position) >= 1: # in case there are multiple goals , needs to be handled for other env types\n",
    "                self.goal_position = (int(self.goal_position[0]/self.height) , self.goal_position[0]%self.width)\n",
    "        return obs\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if  self.goal_position[0] - self.agent_pos[0] == 0:\n",
    "            slope = np.inf\n",
    "        else:\n",
    "            slope = np.divide( self.goal_position[1] - self.agent_pos[1] ,  self.goal_position[0] - self.agent_pos[0])\n",
    "        obs['goal_direction'] = (np.arctan( slope )*(180/np.pi)) if self.type == 'angle' else slope\n",
    "        return obs['goal_direction']\n",
    "\n",
    "class FourDirectionsActionWrapper(gym.core.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "    def action(self, action):\n",
    "        while self.env.agent_dir != action:\n",
    "            self.env.step(0)        \n",
    "        return 2\n",
    "\n",
    "\n",
    "class CoordsObsWrapper(gym.core.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(0, max(env.width, env.height)-3, shape=(2,), dtype=np.int32)\n",
    "        \n",
    "\n",
    "    def observation(self, obs):\n",
    "        return self.agent_pos[1], self.agent_pos[0]\n",
    "\n",
    "\n",
    "class RewardWrapper(gym.core.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step_count += 1\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Get the position in front of the agent\n",
    "        fwd_pos = self.env.front_pos\n",
    "\n",
    "        # Get the contents of the cell in front of the agent\n",
    "        fwd_cell = self.env.grid.get(*fwd_pos)\n",
    "\n",
    "        # Rotate left\n",
    "        if action == self.env.actions.left:\n",
    "            self.env.agent_dir -= 1\n",
    "            if self.env.agent_dir < 0:\n",
    "                self.env.agent_dir += 4\n",
    "\n",
    "        # Rotate right\n",
    "        elif action == self.env.actions.right:\n",
    "            self.env.agent_dir = (self.env.agent_dir + 1) % 4\n",
    "\n",
    "        # Move forward\n",
    "        elif action == self.env.actions.forward:\n",
    "            if fwd_cell == None or fwd_cell.can_overlap():\n",
    "                self.env.agent_pos = fwd_pos\n",
    "            if fwd_cell != None and fwd_cell.type == 'goal':\n",
    "                done = True\n",
    "                reward = 1\n",
    "            if fwd_cell != None and fwd_cell.type == 'lava':\n",
    "                reward = -2\n",
    "                done = True\n",
    "\n",
    "        # Pick up an object\n",
    "        elif action == self.env.actions.pickup:\n",
    "            if fwd_cell and fwd_cell.can_pickup():\n",
    "                if self.env.carrying is None:\n",
    "                    self.env.carrying = fwd_cell\n",
    "                    self.env.carrying.cur_pos = np.array([-1, -1])\n",
    "                    self.env.grid.set(*fwd_pos, None)\n",
    "\n",
    "        # Drop an object\n",
    "        elif action == self.env.actions.drop:\n",
    "            if not fwd_cell and self.env.carrying:\n",
    "                self.env.grid.set(*fwd_pos, self.env.carrying)\n",
    "                self.env.carrying.cur_pos = fwd_pos\n",
    "                self.env.carrying = None\n",
    "\n",
    "        # Toggle/activate an object\n",
    "        elif action == self.env.actions.toggle:\n",
    "            if fwd_cell:\n",
    "                fwd_cell.toggle(self.env, fwd_pos)\n",
    "\n",
    "        # Done action (not used by default)\n",
    "        elif action == self.env.actions.done:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            assert False, \"unknown action\"\n",
    "\n",
    "        if self.env.step_count >= self.env.max_steps:\n",
    "            done = True\n",
    "\n",
    "        obs = self.env.gen_obs()\n",
    "        reward += -0.001\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self, random_start=False):\n",
    "        # Current position and direction of the agent\n",
    "        self.env.agent_pos = None\n",
    "        self.env.agent_dir = None\n",
    "\n",
    "        # Generate a new random grid at the start of each episode\n",
    "        # To keep the same grid for each episode, call env.seed() with\n",
    "        # the same seed before calling env.reset()\n",
    "        self.env._gen_grid(self.env.width, self.env.height)\n",
    "\n",
    "        # These fields should be defined by _gen_grid\n",
    "        assert self.env.agent_pos is not None\n",
    "        assert self.env.agent_dir is not None\n",
    "\n",
    "        if random_start:\n",
    "            self.env.agent_pos = (random.randint(1,self.env.width-2),random.randint(1,self.env.height-2))\n",
    "            start_cell = self.env.grid.get(*self.env.agent_pos)\n",
    "            while start_cell is not None:\n",
    "                self.env.agent_pos = (random.randint(1,self.env.width-2),random.randint(1,self.env.height-2))\n",
    "                start_cell = self.env.grid.get(*self.env.agent_pos)\n",
    "\n",
    "        # Check that the agent doesn't overlap with an object\n",
    "        start_cell = self.env.grid.get(*self.env.agent_pos)\n",
    "        assert start_cell is None or start_cell.can_overlap()\n",
    "\n",
    "        # Item picked up, being carried, initially nothing\n",
    "        self.env.carrying = None\n",
    "\n",
    "        # Step count since episode start\n",
    "        self.env.step_count = 0\n",
    "\n",
    "        # Return first observation\n",
    "        obs = self.env.gen_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90622048",
   "metadata": {},
   "source": [
    "# Getting Started with OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ec867",
   "metadata": {},
   "source": [
    "Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.\n",
    "<!-- ![alt text](inkscape/basic_rl_1.png \"Title\") -->\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/basic_rl_1.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005035c",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87e2c3",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "The most important attributes are the action space and observation space. These define how the agent can interact with the environmnet and the type of observation that can be expected from the environment.\n",
    "\n",
    "`action space`\n",
    "The action space defines the available actions that can be performed in the environment\n",
    "\n",
    "`observation space`\n",
    "The observation space defines the observation the evironment returns.\n",
    "\n",
    "\n",
    "\n",
    "### Methods\n",
    "`def step(action)` performs a step in the environment and returns an observation, reward, and if the episode has terminated. \n",
    "\n",
    "`def reset()` resets the environment.\n",
    "\n",
    "`def render` renders the environment.\n",
    "\n",
    "### Wrappers\n",
    "Sometimes the action space and observation space of an environment can be too complex for an agent to utilise. Wrappers are used to transform an environment in a modular way. Environment wrappers are used to transform an observation before returning it the agent or to transform actions before performing them in the environment. Environment wrappers can also be used to tranform or the reward function of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc47c1",
   "metadata": {},
   "source": [
    "## Gym Minigrid \n",
    "[Gym Minigrid](https://github.com/maximecb/gym-minigrid) is a third party gym environment developed by Chevalier-Boisvert. Gym  Minigrid has various environments to choose from with different goals and difficulties. We will utilse this environment in the rest of the notebook. Some examples of available environments are:\n",
    "<ol>\n",
    "    <li>MiniGrid-Empty-8x8-v0</li>\n",
    "    <li>MiniGrid-DistShift1-v0</li>\n",
    "    <li>MiniGrid-DistShift2-v0</li>\n",
    "    <li>MiniGrid-LavaCrossingS9N3-v0</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc3930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.561365Z",
     "start_time": "2021-09-29T07:11:28.558898Z"
    }
   },
   "outputs": [],
   "source": [
    "env_list = ['MiniGrid-Empty-8x8-v0',\n",
    "            'MiniGrid-UnlockPickup-v0',\n",
    "            'MiniGrid-DistShift1-v0',\n",
    "            'MiniGrid-LavaCrossingS9N3-v0'\n",
    "           ]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,7),)\n",
    "plt.subplots_adjust(bottom=-0.1)\n",
    "axs = axs.flatten()\n",
    "for env_name, ax in zip(env_list, axs):\n",
    "    env = gym.make(env_name) # create environment\n",
    "    env.reset()\n",
    "    img = env.render(mode='rgb', highlight=False)\n",
    "    ax.imshow(img, interpolation='nearest')\n",
    "    ax.set_title(env_name +'\\nMission: ' + env.mission)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23731f",
   "metadata": {},
   "source": [
    "Actions in the basic Minigrid environment:\n",
    "<ol>\n",
    "<li>Turn left</li>\n",
    "<li>Turn right</li>\n",
    "<li>Move forward</li>\n",
    "<li>Pick up an object</li>\n",
    "<li>Drop the object being carried</li>\n",
    "<li>Toggle (open doors, interact with objects)</li>\n",
    "<li>Done (task completed, optional)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ddf97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.569779Z",
     "start_time": "2021-09-29T07:11:28.562702Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name = env_list[2]\n",
    "env = gym.make(env_name) # create environment\n",
    "env.reset()\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd605e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.576059Z",
     "start_time": "2021-09-29T07:11:28.570750Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Observation space: {env.observation_space}') # observation space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76821c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Action space: {env.action_space}') # action space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b00293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying wrappers to the environment\n",
    "env = RewardWrapper(env)\n",
    "env = CoordsObsWrapper(env) # wrapper that returns coordinates as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrapper that transforms action space to a four direction action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603716d",
   "metadata": {},
   "source": [
    "### The reward function\n",
    "A custom reward function is used in this notebook. The agent receives a reward of +1 if it gets to the green goal. We argued dying is worse than getting to the goal, therefore if the agent falls in the lava it receives a reward of -2. The agent receives an reward of -0.001 for each step it takes in the environment to incentivise it to complete the task as quickly as possible.\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/reward_function.png\" width=\"450\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b664384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Observation space: {env.observation_space}') # observation space of the environment\n",
    "print(f'Random observation: {env.observation_space.sample()}') # sample random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f418b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Action space: {env.action_space}') # action space of the environment\n",
    "print(f'Random action: {env.action_space.sample()}') # sample random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92110e",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Reinforcement learning (RL) tries to solve a sequential decision-making problem, which is usually modelled as a Markov decision process (MDP). RL is concerned with how to choose actions from situations to maximise some notion of a numerical reward signal. The reward signal is a measure of how good the performed action or sequence of actions was, and it is used to guide software agents to learn optimal behaviour in a given\n",
    "situation. In RL, the idea of trial and error is central, since the agent is not instructed\n",
    "which actions to take and it has no prior knowledge of the task at hand. The agent aims to\n",
    "discover the consequences of actions through experimentation. Sometimes an action may\n",
    "not only have an impact on the agent’s current situation but may also have an impact on\n",
    "the subsequent situations that the agent encounters. Thus, the aspect of delayed reward\n",
    "signals is also an important concept, as the outcome of specific actions can be experienced\n",
    "at a later time.\n",
    "\n",
    "The environment is the world which is assigned to the agent. It can be set in the\n",
    "physical world, or it can be in the form of a computer simulator. The latter is very popular\n",
    "to use for testing RL algorithms as simulations can be run at accelerated speeds.\n",
    "The environment characterises the task at hand by defining the actions available, the\n",
    "outcome of an action taken in a given situation, and the reward signal function. The\n",
    "current environmental situation is known as the state of the environment.\n",
    "The agent begins by receiving the initial state of the environment. The agent reacts to\n",
    "the state by sending an action to the environment; in return, the agent receives a new\n",
    "state and reward. The action of the agent is based on the observation received and chosen\n",
    "according to the agent’s policy, where the policy determines the behaviour of the agent.\n",
    "The process of acting and receiving observations and rewards in return is recurrent. At\n",
    "each iteration of this process, the agent adjusts its policy in order to receive more rewards.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/agent_environment_interaction.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f275d01",
   "metadata": {},
   "source": [
    "### Interacting with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2e9ed-6de2-42e1-a92a-3064fb257d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.670151Z",
     "start_time": "2021-09-29T07:11:28.576988Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = env.reset() # reset and get an initial observation\n",
    "action = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample() # sample a random action from the action space\n",
    "    obs, reward, done, _ = env.step(action) # perform the action and receive an observation, reward, and if the episode is done\n",
    "    render(env) # render the environment\n",
    "    if done: # if the episode has terminated\n",
    "        env.reset() # reset the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9d0a5",
   "metadata": {},
   "source": [
    "## Rewards and Returns\n",
    "\n",
    "The goal of RL agent is described in terms of the reward signal that it receives at every time step.\n",
    "The reward signal is a real number, $R_t \\in \\mathbb{R}$ and the objective of the agent is to maximise the sum of rewards received over time, i.e. the cumulative sum of rewards received in the course of its trajectory and not just the immediate reward.\n",
    "The sum of all the rewards received during a trajectory in MDP is called the *return*, denoted with $G_t$.\n",
    "In the simplest case, the return $G_t$ of a trajectory is the sum of rewards received after the time step $t$,\n",
    "\\begin{equation}\n",
    "    G_t  \\doteq R_{t+1} +  R_{t+2} + R_{t+3} + \\dots+ R_T,\n",
    "        % & = \\sum_{k=0}^{T-1} \\gamma^k R_{t+k+1},\n",
    "\\end{equation}\n",
    "where $T$ is the final time step of the trajectory.\n",
    "We now also discuss the idea of discounting rewards.\n",
    "By using this approach, the agent's objective is to choose actions in order to maximise the expected sum of discounted rewards.\n",
    "Sutton and Barto define the discounted return $G_t$ as\n",
    "\\begin{equation}\\label{eq:returndiscounted}\n",
    "\\begin{aligned}\n",
    "    G_t & \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\n",
    "    & = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Gamma $\\gamma \\in [0,1]$ is the *discount rate* and determines the value of future rewards by scaling rewards based on the time step when it is acquired.\n",
    "A value close to 0 favours rewards that are acquired in the near future, which leads to a *myopic* evaluation, whereas a value close to 1 assigns equal importance to all future rewards and leads to a more *far-sighted* evaluation.\n",
    "Discounting rewards allows us to favour rewards based on the time they are received.\n",
    "It also allows the return $G_t$ to be finite in tasks where $T=\\infty$.\n",
    "Note that there is a recursive relationship between returns at successive time steps\n",
    "\\begin{equation}\\label{eq:recursivereturn}\n",
    "    \\begin{aligned}\n",
    "    G_t & \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots \\\\\n",
    "    & = R_{t+1} + \\gamma (R_{t+2}+\\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\dots)\\\\\n",
    "    & = R_{t+1} + \\gamma G_{t+1}.\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "This recursive relationship is fundamental to RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48304dd9",
   "metadata": {},
   "source": [
    "## Policies and Value Functions\n",
    "\n",
    "In MDP the agent has to sequentially choose actions to accumulate rewards with the goal to receive the highest possible return.\n",
    "Unfortunately a fixed sequence of actions will not solve MDP due to the uncertainty that is present in the transition model. \n",
    "The agent therefore needs to find a solution where each state in the MDP is mapped to a probability distribution that gives the probability of choosing each possible action.\n",
    "Such a solution is described by Sutton and Barto as a stochastic rule which selects actions as a function of states, and is defined as policy $\\pi$.\n",
    "If the agent is following a policy $\\pi$ at time step $t$, then $\\pi(a|s)$ is the probability of taking the action $a$ given the state $s$.\n",
    "The state-value function $v_\\pi(s)$ is a way to evaluate the quality of a policy.\n",
    "The state-value function $v_\\pi(s)$ is the expected return $G_t$ if the agent is in state $s$ and then follows a policy $\\pi$ and is defined by Sutton and Barto as\n",
    "\\begin{equation}\\label{eq:statevalue}\n",
    "\\begin{aligned}\n",
    "    v_\\pi(s) & \\doteq \\mathbb{E}_\\pi\\left[G_t|S_t = s\\right]\\\\\n",
    "            &= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\middle|S_t = s\\right], \\text{ for all } s \\in \\mathcal{S}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The state-value function $v_\\pi(s)$ is thus an estimate of how good it is to be in a given state.\n",
    "The action-value function $q_\\pi(s,a)$ is another way to evaluate a policy $\\pi$. The action-value function $q_\\pi(s,a)$ is the expected return $G_t$ if the agent is in state $s$, takes action $a$ and then follows policy $\\pi$. Sutton and Barto define it as\n",
    "\\begin{equation}\\label{eq:actionvalue}\n",
    "\\begin{aligned}\n",
    "    q_\\pi(s,a) & \\doteq \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\\\\\n",
    "               &= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\middle|S_t = s, A_t = a\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The action-value function $q_\\pi(s,a)$ is therefore an estimate of how beneficial it is to perform a specified action in a given state and then following the policy $\\pi$ afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a87e6",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Evaluation\n",
    "\n",
    "We first discuss how MC methods can be used for prediction or policy evaluation.\n",
    "It has been shown by Sutton and Barto that if the agent follows a policy $\\pi$ and encounters a state $s$, the average return that follows the state $s$  will converge to the state's value $v_\\pi(s)$, as the number of times the state is visited approaches infinity. \n",
    "If an average is tracked for each action taken in the state $s$, then the state-action value $q_\\pi(s,a)$ can similarly be determined.\n",
    "These methods of estimating the value functions are called *Monte Carlo methods* because they entail averaging over many random sample episodes using the actual returns received.\n",
    "As MC methods use samples of entire episodes for policy evaluation, the estimate of the value function is unbiased.\n",
    "The drawback is that MC methods have to wait until the return $G_t$ is known.\n",
    "Therefore values of states can only be updated at the end of an episode.\n",
    "\n",
    "Each time a state $s$ is encountered we refer to it as a *visit* to $s$.\n",
    "The first time the state $s$ is encountered, we refer to as the *first visit* to $s$.\n",
    "There are two main MC policy evaluation methods to estimate $v_\\pi$, namely the *first-visit* MC method and the *every-visit* MC method.\n",
    "It has been shown by Sutton and Barto that the first-visit MC method estimates $v_\\pi(s)$ as the mean return following the first visits to $s$.\n",
    "On the other hand, the every-visit MC method estimates $v_\\pi(s)$ as the mean return following all visits to $s$.\n",
    "According to Sutton and Barto, first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of times $s$ is encountered, approaches infinity.\n",
    "The result is an unbiased estimate of the expected value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d02bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.681774Z",
     "start_time": "2021-09-29T07:11:35.671828Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TODO: Rather a class than a function?\n",
    "def MCPE(episodes, env):\n",
    "    returns = np.zeros((env.height, env.width))\n",
    "    v_count = np.zeros((env.height, env.width))\n",
    "    value_function = np.ones((env.height, env.width))*-2\n",
    "\n",
    "    for i in range(episodes):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        env.seed(1)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            states.append(state)\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            rewards.append(reward)\n",
    "\n",
    "        G = 0\n",
    "        for state, reward in zip(states[::-1], rewards[::-1]):\n",
    "            G += reward\n",
    "            v_count[state] += 1\n",
    "            returns[state] += G\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(figsize=(8,5))\n",
    "            ax = sns.heatmap(value_function, cmap='RdYlGn', alpha = 0.8, zorder=2, annot = True, fmt='.3f')\n",
    "            img = env.grid.render(tile_size=32)\n",
    "            ax.imshow(img,\n",
    "                      aspect = ax.get_aspect(),\n",
    "                      extent = ax.get_xlim() + ax.get_ylim(),\n",
    "                      interpolation='nearest', zorder = 1)\n",
    "            plt.pause(0.00001)\n",
    "\n",
    "        value_function = np.round(returns/v_count, 3)\n",
    "\n",
    "    return np.round(returns/v_count, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.858361Z",
     "start_time": "2021-09-29T07:11:35.683449Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MiniGrid-DistShift2-v0') # create environment\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "value_function = MCPE(2000, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00b02f",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "Using dynamic programming a model of the environment was required to perform a greedy policy improvement over a state-value function.\n",
    "A greedy policy improvement over $v_\\pi(s)$ is given by\n",
    "\\begin{equation}\\label{eq:statevalueimprovement}\n",
    "    \\pi'(s) = \\underset{a}{\\text{argmax}} \\sum_{s',r}p(s',r|s,a)\\big[r+ \\gamma v_\\pi(s')\\big].\n",
    "\\end{equation}\n",
    "It is clear that the probabilities $p(s',r|s,a)$, which characterise the dynamics of the environment, are required to perform the policy improvement.\n",
    "It is therefore impossible to use the state-value function for policies improvement steps in model-free settings.\n",
    "In these applications, the action-value function is handy as it can be used to improve a policy without having access to the environment's model.\n",
    "A greedy policy improvement over $q_\\pi(s,a)$ is given by\n",
    "\\begin{equation}\n",
    "    \\pi'(s) = \\underset{a}{\\text{argmax }}q_\\pi(s,a).\n",
    "\\end{equation}\n",
    "We therefore use the action-value function in MC control in order to do policy improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2844f",
   "metadata": {},
   "source": [
    "## ε-Greedy Exploration\n",
    "Acting *greedily* with regard to the current estimated action-value function allows us to exploit our current knowledge of the system and allows for the maximisation of the expected reward on the current step.\n",
    "By selecting nongreedy actions, alternative actions are explored and allow for improving the estimates of values of the nongreedy actions.\n",
    "Alternative actions may be better than what current knowledge estimates to be best.\n",
    "After discovering better actions, these can be exploited to produce greater reward in the long run.\n",
    "There is, therefore, a conflict between exploration and exploitation as it is not possible to perform both with a single action.\n",
    "\n",
    "MC methods use samples of experienced episodes to update value functions and to improve policies. \n",
    "Sufficiently exploring all possible state-action pairs with MC methods can be problematic.\n",
    "By selecting only actions that are estimated to be the best, i.e. always performing greedy actions, alternative actions, which may be better, are never selected.\n",
    "We must therefore ensure that the algorithm continues to explore alternative state-action pairs.\n",
    "According to Sutton and Barto we can solve this problem by starting episodes with randomly sampled state-action pairs (with all state-action pairs having a none-zero probability of being sampled).\n",
    "This guarantees that all state-actions pairs will be encountered an infinite number of times for an infinite number of episodes.\n",
    "Sutton and Barto define the above assumption of randomly initialising state-action pairs at the start of an episode as *exploring starts*.\n",
    "In simulated environments, exploring starts can often be implemented, but this method is usually not feasible to use with real-world problems.\n",
    "\n",
    "An alternative way to address the problem of maintaining exploration is to have a stochastic policy that has a non-zero probability of selecting all possible actions in each state.\n",
    "% We, therefore, review the *$\\varepsilon$-greedy* exploration strategy.\n",
    "In applications where exploring starts cannot be used, the *$\\varepsilon$-greedy* exploration strategy is convenient to use.\n",
    "It is a very simple, effective exploration strategy that ensures continual exploration~\\cite{mnih2013playing}.\n",
    "\\citet{website:david_silver} defines the $\\varepsilon$-greedy strategy as\n",
    "\\begin{equation}\n",
    "    \\pi(a|s) = \\begin{cases} \\varepsilon/|\\mathcal{A}(S_t)| +1-\\varepsilon &\\text{if }a^* = \\underset{a}{\\text{argmax }}Q(s,a) \\\\\n",
    "                        \\varepsilon/|\\mathcal{A}(S_t)| &\\text{otherwise}\n",
    "    \\end{cases},\n",
    "\\end{equation}\n",
    "where $|\\mathcal{A}(S_t)|$ is the cardinality of the action space.\n",
    "Every time the agent performs an action, it has a probability of $1-\\varepsilon$ of selecting the greedy action, else (with a probability of $\\varepsilon$) the action is randomly sampled.\n",
    "\n",
    "<!-- <ol>\n",
    "<li>Choosing always what is believed to be optimal may cause that better states are never explored</li>\n",
    "<li>A small probability to perform a random action, the rest of the time perform the greedy action</li>\n",
    "<li>Probability 1 − ε  choose the greedy action</li>\n",
    "<li>Probability ε choose an action at random</li>\n",
    "</ol>\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/e-greedy.png\" width=\"600\">\n",
    "</p> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c0339",
   "metadata": {},
   "source": [
    "### Implement ε-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, qtable, state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(qtable[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3b1eb",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/monte_carlo_policy_iteration.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/monte_carlo_control.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "transition = namedtuple('transition', 'state action')\n",
    "\n",
    "def MC_Control_1(episodes, env, epsilon):\n",
    "    q_table = defaultdict(lambda: np.zeros(shape=(env.action_space.n,)))\n",
    "    returns = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        episode = []\n",
    "        rewards = []\n",
    "        env.seed(1)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:            \n",
    "            action = e_greedy(env, q_table, state, 0.1)\n",
    "            episode.append((state, action))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        G = 0\n",
    "        for (state, action), reward in zip(episode[::-1], rewards[::-1]):\n",
    "            G += reward\n",
    "            returns[state][action].append(G)\n",
    "            q_table[state][action] = np.mean(returns[state][action])\n",
    "        if ep % 100 == 0:\n",
    "            clear_output(wait = True)\n",
    "            fig, ax = plt.subplots()\n",
    "            plot_q_value_func(ax, q_table, env)\n",
    "            plt.pause(0.00001)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d53a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "transition = namedtuple('transition', 'state action')\n",
    "\n",
    "def MC_Control_2(episodes, env, epsilon):\n",
    "    q_table = defaultdict(lambda: np.zeros(shape=(env.action_space.n,)))\n",
    "    returns = defaultdict(lambda: np.zeros(shape=(env.action_space.n,)))\n",
    "    visits = defaultdict(lambda: np.zeros(shape=(env.action_space.n,)))\n",
    "\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        episode = []\n",
    "        rewards = []\n",
    "        env.seed(1)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:            \n",
    "            action = e_greedy(env, q_table, state, 0.1)\n",
    "            episode.append((state, action))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        G = 0\n",
    "        for (state, action), reward in zip(episode[::-1], rewards[::-1]):\n",
    "            G += reward\n",
    "            returns[state][action] += G\n",
    "            visits[state][action] +=1\n",
    "            q_table[state][action] = returns[state][action]/visits[state][action]\n",
    "        if ep % 100 == 0:\n",
    "            clear_output(wait = True)\n",
    "            fig, ax = plt.subplots()\n",
    "            plot_q_value_func(ax, q_table, env)\n",
    "            plt.pause(0.00001)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MiniGrid-DistShift1-v0') # create environment\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "q_table = MC_Control_1(1000, env, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(epsilon, env, qtable):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    for i in range(5):\n",
    "        print(i)\n",
    "        while not done:\n",
    "            action = e_greedy(env, qtable, state,epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            render(env)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "demo(0, env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9226e0b",
   "metadata": {},
   "source": [
    "## Temporal Difference Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d88d",
   "metadata": {},
   "source": [
    "# Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd9504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.764720Z",
     "start_time": "2021-09-29T07:12:05.750714Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Base Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name,\n",
    "                 learning_rate=0.1,\n",
    "                 eps_start = 1,\n",
    "                 eps_end= 0.1,\n",
    "                 eps_end_episode = 100,\n",
    "                 episodes=1000,\n",
    "                 gamma=0.95):\n",
    "        self.action_space = ['right', 'down', 'left', 'up']\n",
    "        self.env_name = env_name\n",
    "        self.env = self.create_env(env_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mode = 'rgb'\n",
    "        self.size = 32\n",
    "        \n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_end_episode = eps_end_episode        \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        num_actions = self.env.action_space.n\n",
    "        self.table = defaultdict(lambda: np.zeros(shape=(num_actions,)))\n",
    "        \n",
    "        self.episode_return_history = []\n",
    "        self.epsilon_history = []\n",
    "        \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.eps_end, \n",
    "                    ((self.eps_end - self.eps_start) / self.eps_end_episode)*episode \n",
    "                    + self.eps_start)\n",
    "        \n",
    "    def create_env(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        env = RewardWrapper(env)\n",
    "        env = CoordsObsWrapper(env)\n",
    "        env = FourDirectionsActionWrapper(env)\n",
    "        return env        \n",
    "\n",
    "    def e_greedy(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.table[state])\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def log_metrics(self, episode_return, epsilon):\n",
    "        self.episode_return_history.append(episode_return)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        \n",
    "    def plot(self):\n",
    "        clear_output(wait = True)   \n",
    "        plt.figure(figsize=(15,10)) \n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        ax3 = plt.subplot(2,1,2)\n",
    "        reward_range = self.env.reward_range\n",
    "        plot_return(ax1, self.episode_return_history, y_min=reward_range[0]*1.1, y_max= reward_range[1]*1.1)\n",
    "        plot_epsilon(ax2, self.epsilon_history)\n",
    "        plot_q_value_func(ax3, self.table, self.env)        \n",
    "        plt.pause(0.000001)\n",
    "        \n",
    "    def demo_plot(self, state, pause):\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(15,5), gridspec_kw={'width_ratios': [2, 1]}) \n",
    "        img = self.env.render(mode='rgb', highlight=False)\n",
    "        plot_q_value_func(ax1, self.table, self.env, alpha = 0.4, img = img)\n",
    "        plt.ylim(-0.1, 1)\n",
    "        \n",
    "        ax2.set_ylabel('Q-Values')\n",
    "        ax2.set_title('Q-Values')\n",
    "        sns.barplot(x=['right', 'down', 'left', 'up'], y=self.table[state], ax = ax2)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(pause)\n",
    "\n",
    "    def demo(self, epsilon, episodes=5, pause=0.1, random_start = True):\n",
    "        done = False\n",
    "        self.env.seed(1)\n",
    "        state = self.env.reset(random_start = random_start)\n",
    "        for i in range(episodes):\n",
    "            while not done:\n",
    "                self.demo_plot(state, pause)\n",
    "                action = self.e_greedy(state,epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "            self.env.seed(1)\n",
    "            state = self.env.reset(random_start = random_start)\n",
    "            done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239461ae",
   "metadata": {},
   "source": [
    "## SARSA (state, action, reward, state, action)\n",
    "We now review a model-free control TD method, in order to obtain optimal value functions and optimal policies.\n",
    "According to Sutton and  Barto, the idea of TD learning methods is that the approximate policy and approximate value function can interact with each other in such a way that they both move towards their optimal values.\n",
    "The first part of this process is to estimate the value function to predict the returns of the current policy.\n",
    "The second part is to improve the policy with regard to the estimated value function.\n",
    "\n",
    "In this section, we review an on-policy TD control method, *SARSA*.\n",
    "Again the action-value function $q_\\pi$ is used instead of the state-value function $v_\\pi$, as the former does not require a transition model of the environment to improve the policy.\n",
    "The goal is to obtain an optimal action-value function $q_*$ and therefore also an optimal policy $\\pi_*$.\n",
    "We first discuss how a policy $\\pi$ is evaluated, and then how the policy is improved.\n",
    "\n",
    "Similarly to the previous section, we learn a value function from samples of experience.\n",
    "Here state-action pair to state-action pair is considered, and the estimated values of state-action pairs are updated.\n",
    "The estimated action-value $Q(S_t, A_t)$ is updated:\n",
    "\\begin{equation}\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\big].\n",
    "\\end{equation}\n",
    "The update is done for non-terminal states, and if state $S_{t+1}$ is terminal then $Q(S_{t+1}, A_{t+1})$ is defined as zero.\n",
    "\n",
    "The estimated action-value function $Q$ is then used to improve the policy.\n",
    "An $\\varepsilon$-greedy strategy can be used to choose the action $A_{t+1}$ from the state $S_{t+1}$ using the estimated action-value function $Q$.\n",
    "The $\\varepsilon$-greedy strategy acts mostly greedy with regard to the estimated action-value function but also explores nongreedy actions from time to time.\n",
    "In every update $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$ are present and these give rise to the name SARSA.\n",
    "SARSA for on-policy control since the agent learns about the policy it is following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b93fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.771560Z",
     "start_time": "2021-09-29T07:12:05.765796Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentSarsa(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_sarsa(self, action, state, next_state, next_action, reward, done):\n",
    "        v_estimate = 0 if done else self.gamma*self.table[next_state][next_action]        \n",
    "        self.table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        action = self.e_greedy(state, self.get_epsilon(0))\n",
    "\n",
    "        episode_return = 0\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep)\n",
    "            while not done:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                if not done:\n",
    "                    next_action = self.e_greedy(next_state, epsilon)\n",
    "                self.update_sarsa(action, state, next_state,\n",
    "                                  next_action, reward, done)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            state = self.env.reset()\n",
    "            action = self.e_greedy(state, epsilon)\n",
    "            done = False\n",
    "            \n",
    "            self.log_metrics(episode_return, epsilon)\n",
    "            episode_return = 0\n",
    "            \n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde6ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:23:24.055111Z",
     "start_time": "2021-09-29T07:12:05.773670Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa = AgentSarsa(env_name='MiniGrid-DistShift2-v0') #\n",
    "agent_sarsa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca3ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.309684Z",
     "start_time": "2021-09-29T07:23:24.056397Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa.demo(0, pause =0.1, random_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdffc2",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "<!-- <p align=\"center\">\n",
    "<img src=\"resources/q-learning.svg\">\n",
    "</p> -->\n",
    "Q-learning is an off-policy TD method which Sutton and Barto describe as a breakthrough in RL.\n",
    "It is classified as an off-policy method as the behaviour policy is independent of the target policy.\n",
    "The target policy (the policy the agent learns about) is greedy with respect to $Q(s,a)$:\n",
    "\\begin{equation}\n",
    "    \\pi(S_{t+1}) = \\underset{a}{\\text{argmax}} Q(S_{t+1}, a).\n",
    "\\end{equation}\n",
    "The Q-learning target can be simplified to\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    G_t & \\approx R_{t+1} + \\gamma Q\\big(S_{t+1}, \\underset{a}{\\text{argmax}} Q(S_{t+1}, a)\\big)\\\\\n",
    "    &= R_{t+1} + \\gamma \\underset{a}{\\max}  Q(S_{t+1}, a).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Sutton and Barto define the Q-learning update step as\n",
    "\\begin{equation}\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[R_{t+1} + \\gamma \\underset{a}{\\max} Q(S_{t+1}, a) - Q(S_t, A_t) \\big].\n",
    "\\end{equation}\n",
    "The behaviour policy can be anything, but it is usually $\\varepsilon$-greedy with respect to $Q(s,a)$.\n",
    "The behaviour policy can also take the form of experience generated by older policies or by expert demonstrators.\n",
    "The off-policy nature of the algorithm allows it to be more sample-efficient as it can reuse previously generated experience to learn.\n",
    "This also makes it possible to learn from several different policies at the same time.\n",
    "Sutton and Barto state that with Q-learning, the estimate action-value function $Q$ converges to the optimal action-value function $q_*$ when all state-action pairs are infinitely updated.\n",
    "Again, in practice it is not necessary to converge all the way to $q_*$ to obtain an optimal policy $\\pi_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01964e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.328505Z",
     "start_time": "2021-09-29T07:24:05.312569Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentQL(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_q_table(self, action, state, next_state, reward, done):\n",
    "        v_estimate = 0 if done else self.gamma*np.max(self.table[next_state])\n",
    "        self.table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        done = False\n",
    "        self.env.seed(1)\n",
    "        state = self.env.reset()\n",
    "        step = 0\n",
    "        episode_return = 0\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep)\n",
    "            while not done:\n",
    "                action = self.e_greedy(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                self.update_q_table(action, state, next_state, reward, done)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            self.env.seed(1)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            self.log_metrics(episode_return, epsilon)\n",
    "            episode_return = 0\n",
    "            \n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeb547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.614861Z",
     "start_time": "2021-09-29T07:24:05.330277Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q = AgentQL(env_name='MiniGrid-DistShift2-v0')\n",
    "agent_q.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c7042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.618202Z",
     "start_time": "2021-09-29T07:34:51.615936Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q.demo(0,pause=0.05, episodes=10, random_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a80748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.725028Z",
     "start_time": "2021-09-29T07:34:51.619733Z"
    }
   },
   "outputs": [],
   "source": [
    "dis_df = pd.DataFrame({'SARSA': agent_sarsa.episode_return_history, 'Q-Learning': agent_q.episode_return_history})\n",
    "sns.displot(dis_df, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_q = AgentQL(env_name='MiniGrid-Empty-8x8-v0')\n",
    "agent_q.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae845da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_q.env.reward_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded04e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
