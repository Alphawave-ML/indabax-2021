{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564f41e-fb34-4e8b-90c9-291f61d21543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.557719Z",
     "start_time": "2021-09-29T07:11:27.823599Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import gym\n",
    "import pandas as pd\n",
    "import gym_minigrid\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "normalize_qs = Normalize(vmin = -1, vmax = 1)\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    margin:auto;\n",
    "}\n",
    ".prompt \n",
    "    display:none;\n",
    "} \n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bfd55",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def render(env, mode='rgb', ax=None, highlight=False):\n",
    "    img = env.render(mode=mode, highlight=highlight)\n",
    "    if ax is None:\n",
    "        plt.imshow(img, interpolation='nearest')\n",
    "    else:\n",
    "        ax.imshow(img, interpolation='nearest')\n",
    "    clear_output(wait=True)\n",
    "    plt.pause(0.00001)\n",
    "\n",
    "\n",
    "def draw_triangle(ax, p1=[1, 1], p2=[2, 2.5], p3=[3, 1], c='blue', alpha=0.3):\n",
    "    X = np.array([p1, p2, p3])\n",
    "    Y = [c, c, c]\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=0.001, color=Y[:])\n",
    "    t1 = plt.Polygon(X[:3, :], color=Y[0], alpha=alpha)\n",
    "    ax.add_patch(t1)\n",
    "\n",
    "\n",
    "def draw_square(ax,\n",
    "                lb_x,\n",
    "                lb_y,\n",
    "                size,\n",
    "                alpha=1,\n",
    "                l_color='green',  # left\n",
    "                r_color='yellow',  # right\n",
    "                b_color='red',  # down\n",
    "                t_color='blue'):  # up\n",
    "    tr_x = lb_x+size  # top right x\n",
    "    tr_y = lb_y+size  # top right y\n",
    "\n",
    "    md_x = lb_x + size/2  # middle x\n",
    "    md_y = lb_y + size/2  # middle x\n",
    "\n",
    "    width = 10\n",
    "\n",
    "    md_x_plus = md_x + width/2\n",
    "    md_x_minus = md_x - width/2\n",
    "\n",
    "    md_y_plus = md_y + width/2\n",
    "    md_y_minus = md_y - width/2\n",
    "\n",
    "    colormap = cm.get_cmap('RdYlGn')\n",
    "\n",
    "    color_right = colormap(r_color)\n",
    "    color_down = colormap(b_color)\n",
    "    color_left = colormap(l_color)\n",
    "    color_up = colormap(t_color)\n",
    "\n",
    "    draw_triangle(ax,[lb_x+2, md_y], [md_x_minus, md_y_minus+2],\n",
    "                  [md_x_minus, md_y_plus-2], c=color_left, alpha=alpha)  # left\n",
    "    draw_triangle(ax,[tr_x-2, md_y], [md_x_plus, md_y_minus+2],\n",
    "                  [md_x_plus, md_y_plus-2], c=color_right, alpha=alpha)  # right\n",
    "    draw_triangle(ax,[md_x, tr_y-2], [md_x_minus+2, md_y_plus],\n",
    "                  [md_x_plus-2, md_y_plus], c=color_down, alpha=alpha)  # down\n",
    "    draw_triangle(ax,[md_x, lb_y+2], [md_x_minus+2, md_y_minus],\n",
    "                  [md_x_plus-2, md_y_minus], c=color_up, alpha=alpha)  # up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65df484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_return(ax, returns, y_min, y_max, rolling_window=10):\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Return')\n",
    "    ax.set_title('Return versus Episodes')\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.plot(np.array(returns), linestyle='dashed', color='silver', lw=1.5)\n",
    "    ax.plot(pd.Series(returns).rolling(window=rolling_window).mean().values, lw=2)\n",
    "\n",
    "def plot_epsilon(ax, epsilon_history):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Epsilon')\n",
    "    ax.set_title('Epsilon versus Episodes')\n",
    "    ax.plot(epsilon_history)\n",
    "\n",
    "def plot_q_value_func(ax, table, env, alpha = 1, img = None):\n",
    "    df_table = pd.DataFrame(table).T.reset_index()\n",
    "    df_table.columns = ['y', 'x', 'right', 'down', 'left', 'up']\n",
    "    df_table = df_table.sort_values(by=['y','x'])\n",
    "    df_table['lb_x'] = df_table['x']*32\n",
    "    df_table['lb_y'] = df_table['y']*32\n",
    "\n",
    "    df_table[['right', 'down', 'left', 'up']] = normalize_qs(df_table[['right', 'down', 'left', 'up']])\n",
    "    if img is None:\n",
    "        img = env.grid.render(tile_size=32)\n",
    "    ax.imshow(img, interpolation='nearest')\n",
    "    for row in df_table.itertuples():             \n",
    "        draw_square(ax,\n",
    "                    row[-2], # lb_x\n",
    "                    row[-1], # lb_y\n",
    "                    32, \n",
    "                    alpha=alpha, \n",
    "                    r_color=row[3], # right\n",
    "                    b_color=row[4], # down\n",
    "                    l_color=row[5], # left\n",
    "                    t_color=row[6]) # up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489751d",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "from gym.spaces import Box\n",
    "from gym_minigrid.minigrid import Goal\n",
    "\n",
    "\n",
    "class RewardWrapper(gym.core.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.env.step_count += 1\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Get the position in front of the agent\n",
    "        fwd_pos = self.env.front_pos\n",
    "\n",
    "        # Get the contents of the cell in front of the agent\n",
    "        fwd_cell = self.env.grid.get(*fwd_pos)\n",
    "\n",
    "        # Rotate left\n",
    "        if action == self.env.actions.left:\n",
    "            self.env.agent_dir -= 1\n",
    "            if self.env.agent_dir < 0:\n",
    "                self.env.agent_dir += 4\n",
    "\n",
    "        # Rotate right\n",
    "        elif action == self.env.actions.right:\n",
    "            self.env.agent_dir = (self.env.agent_dir + 1) % 4\n",
    "\n",
    "        # Move forward\n",
    "        elif action == self.env.actions.forward:\n",
    "            if fwd_cell == None or fwd_cell.can_overlap():\n",
    "                self.env.agent_pos = fwd_pos\n",
    "            if fwd_cell != None and fwd_cell.type == 'goal':\n",
    "                done = True\n",
    "                reward = 1\n",
    "            if fwd_cell != None and fwd_cell.type == 'lava':\n",
    "                reward = -2\n",
    "                done = True\n",
    "\n",
    "        # Pick up an object\n",
    "        elif action == self.env.actions.pickup:\n",
    "            if fwd_cell and fwd_cell.can_pickup():\n",
    "                if self.env.carrying is None:\n",
    "                    self.env.carrying = fwd_cell\n",
    "                    self.env.carrying.cur_pos = np.array([-1, -1])\n",
    "                    self.env.grid.set(*fwd_pos, None)\n",
    "\n",
    "        # Drop an object\n",
    "        elif action == self.env.actions.drop:\n",
    "            if not fwd_cell and self.env.carrying:\n",
    "                self.env.grid.set(*fwd_pos, self.env.carrying)\n",
    "                self.env.carrying.cur_pos = fwd_pos\n",
    "                self.env.carrying = None\n",
    "\n",
    "        # Toggle/activate an object\n",
    "        elif action == self.env.actions.toggle:\n",
    "            if fwd_cell:\n",
    "                fwd_cell.toggle(self.env, fwd_pos)\n",
    "\n",
    "        # Done action (not used by default)\n",
    "        elif action == self.env.actions.done:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            assert False, \"unknown action\"\n",
    "\n",
    "        if self.env.step_count >= self.env.max_steps:\n",
    "            done = True\n",
    "\n",
    "        obs = self.env.gen_obs()\n",
    "        reward += -0.001\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self, random_start=False):\n",
    "        # Current position and direction of the agent\n",
    "        self.env.agent_pos = None\n",
    "        self.env.agent_dir = None\n",
    "\n",
    "        # Generate a new random grid at the start of each episode\n",
    "        # To keep the same grid for each episode, call env.seed() with\n",
    "        # the same seed before calling env.reset()\n",
    "        self.env._gen_grid(self.env.width, self.env.height)\n",
    "\n",
    "        # These fields should be defined by _gen_grid\n",
    "        assert self.env.agent_pos is not None\n",
    "        assert self.env.agent_dir is not None\n",
    "\n",
    "        if random_start:\n",
    "            self.env.agent_pos = (random.randint(1,self.env.width-2),random.randint(1,self.env.height-2))\n",
    "            start_cell = self.env.grid.get(*self.env.agent_pos)\n",
    "            while start_cell is not None:\n",
    "                self.env.agent_pos = (random.randint(1,self.env.width-2),random.randint(1,self.env.height-2))\n",
    "                start_cell = self.env.grid.get(*self.env.agent_pos)\n",
    "\n",
    "        # Check that the agent doesn't overlap with an object\n",
    "        start_cell = self.env.grid.get(*self.env.agent_pos)\n",
    "        assert start_cell is None or start_cell.can_overlap()\n",
    "\n",
    "        # Item picked up, being carried, initially nothing\n",
    "        self.env.carrying = None\n",
    "\n",
    "        # Step count since episode start\n",
    "        self.env.step_count = 0\n",
    "\n",
    "        # Return first observation\n",
    "        obs = self.env.gen_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92110e",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Reinforcement learning (RL) tries to solve a sequential decision-making problem, which is usually modelled as a Markov decision process (MDP). RL is concerned with how to choose actions from situations to maximise some notion of a numerical reward signal. The reward signal is a measure of how good the performed action or sequence of actions was, and it is used to guide software agents to learn optimal behaviour in a given\n",
    "situation. In RL, the idea of trial and error is central, since the agent is not instructed\n",
    "which actions to take and it has no prior knowledge of the task at hand. The agent aims to\n",
    "discover the consequences of actions through experimentation. Sometimes an action may\n",
    "not only have an impact on the agent’s current situation but may also have an impact on\n",
    "the subsequent situations that the agent encounters. Thus, the aspect of delayed reward\n",
    "signals is also an important concept, as the outcome of specific actions can be experienced\n",
    "at a later time.\n",
    "\n",
    "The environment is the world which is assigned to the agent. It can be set in the\n",
    "physical world, or it can be in the form of a computer simulator. The latter is very popular\n",
    "to use for testing RL algorithms as simulations can be run at accelerated speeds.\n",
    "The environment characterises the task at hand by defining the actions available, the\n",
    "outcome of an action taken in a given situation, and the reward signal function. The\n",
    "current environmental situation is known as the state of the environment.\n",
    "The agent begins by receiving the initial state of the environment. The agent reacts to\n",
    "the state by sending an action to the environment; in return, the agent receives a new\n",
    "state and reward. The action of the agent is based on the observation received and chosen\n",
    "according to the agent’s policy, where the policy determines the behaviour of the agent.\n",
    "The process of acting and receiving observations and rewards in return is recurrent. At\n",
    "each iteration of this process, the agent adjusts its policy in order to receive more rewards.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/agent_environment_interaction.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90622048",
   "metadata": {},
   "source": [
    "# Getting Started with OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ec867",
   "metadata": {},
   "source": [
    "Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.\n",
    "<!-- ![alt text](inkscape/basic_rl_1.png \"Title\") -->\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/basic_rl_1.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005035c",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87e2c3",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "The most important attributes are the action space and observation space. These define how the agent can interact with the environment and the type of observation that can be expected from the environment.\n",
    "\n",
    "`action space`\n",
    "The action space defines the available actions that can be performed in the environment\n",
    "\n",
    "`observation space`\n",
    "The observation space defines the observation the evironment returns.\n",
    "\n",
    "\n",
    "\n",
    "### Methods\n",
    "`def step(action)` performs a step in the environment and returns an observation, reward, and if the episode has terminated. \n",
    "\n",
    "`def reset()` resets the environment.\n",
    "\n",
    "`def render()` renders the environment.\n",
    "\n",
    "### Wrappers\n",
    "Sometimes the action space and observation space of an environment can be too complex for an agent to utilise. Wrappers are used to transform an environment in a modular way. Environment wrappers are used to transform an observation before returning it the agent or to transform actions before performing them in the environment. Environment wrappers can also be used to tranform or the reward function of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc47c1",
   "metadata": {},
   "source": [
    "## Gym Minigrid \n",
    "[Gym Minigrid](https://github.com/maximecb/gym-minigrid) is a third party gym environment developed by Chevalier-Boisvert. Gym  Minigrid has various environments to choose from with different goals and difficulties. We will utilse this environment in the rest of the notebook. Some examples of available environments are:\n",
    "<ol>\n",
    "    <li>MiniGrid-Empty-8x8-v0</li>\n",
    "    <li>MiniGrid-UnlockPickup-v0</li>\n",
    "    <li>MiniGrid-DistShift1-v0</li>\n",
    "    <li>MiniGrid-LavaCrossingS9N3-v0</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc3930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.561365Z",
     "start_time": "2021-09-29T07:11:28.558898Z"
    }
   },
   "outputs": [],
   "source": [
    "env_list = ['MiniGrid-Empty-8x8-v0',\n",
    "            'MiniGrid-UnlockPickup-v0',\n",
    "            'MiniGrid-DistShift1-v0',\n",
    "            'MiniGrid-LavaCrossingS9N3-v0'\n",
    "           ]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,7),)\n",
    "plt.subplots_adjust(bottom=-0.1)\n",
    "axs = axs.flatten()\n",
    "for env_name, ax in zip(env_list, axs):\n",
    "    env = gym.make(env_name) # create environment\n",
    "    env.reset()\n",
    "    img = env.render(mode='rgb', highlight=False)\n",
    "    ax.imshow(img, interpolation='nearest')\n",
    "    ax.set_title(env_name +'\\nMission: ' + env.mission)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23731f",
   "metadata": {},
   "source": [
    "In this notebook we will focus on basic environment where the goal of the agent is to get to the green goal while avoiding the lava. Let's create the 'MiniGrid-DistShift1-v0' environment and render it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ddf97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.569779Z",
     "start_time": "2021-09-29T07:11:28.562702Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'MiniGrid-DistShift1-v0'\n",
    "env = gym.make(env_name) # create environment\n",
    "env.reset() # reset the environment\n",
    "render(env, highlight=True) # call helper function to render the environment in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daba878",
   "metadata": {},
   "source": [
    "Next we have a look at the observation space and action space of the environment. This is important since our agent will have to interperate the observations to choose actions to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd605e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.576059Z",
     "start_time": "2021-09-29T07:11:28.570750Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Observation space: {env.observation_space}') # observation space of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c578e5",
   "metadata": {},
   "source": [
    "The default observation the environment returns is of type `gym.spaces.Box`. It returns a numpy array with a shape of (7, 7, 3) with a minimum value of 0 and maxiumum value of 255. This represents and RGB image of the agent's field of view.\n",
    "\n",
    "Let's view the action space of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76821c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Action space: {env.action_space}') # action space of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9312be6",
   "metadata": {},
   "source": [
    "The action space is of type `gym.spaces.Discrete`. The agent can perform an action with a discrete value between [0, 7).\n",
    "The actions of the gym Minigrid environment are defined as follows:\n",
    "<ol>\n",
    "<li>Turn left</li>\n",
    "<li>Turn right</li>\n",
    "<li>Move forward</li>\n",
    "<li>Pick up an object</li>\n",
    "<li>Drop the object being carried</li>\n",
    "<li>Toggle (open doors, interact with objects)</li>\n",
    "<li>Done (task completed, optional)</li>\n",
    "</ol>\n",
    "\n",
    "To sample a random action from the action space call `env.action_space.sample()`. Let's use this function to perform random actions in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2e9ed-6de2-42e1-a92a-3064fb257d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.670151Z",
     "start_time": "2021-09-29T07:11:28.576988Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = env.reset() # reset and get an initial observation\n",
    "action = 0\n",
    "\n",
    "import time\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample() # sample a random action from the action space\n",
    "    obs, reward, done, _ = env.step(action) # perform the action and receive an observation, reward, and if the episode is done\n",
    "    \n",
    "    # render the environment\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(9,10), gridspec_kw={'width_ratios': [2, 1]})\n",
    "    ax1.set_title('Entire environment')\n",
    "    ax2.set_title(\"Agent's observation\")\n",
    "    ax2.imshow(env.get_obs_render(obs['image']), interpolation='nearest') # render the agent's observation\n",
    "    render(env, ax=ax1,highlight=True) # render the environment\n",
    "    \n",
    "    if done: # if the episode has terminated\n",
    "        env.reset() # reset the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f2b0f",
   "metadata": {},
   "source": [
    "Our first agent engagef with the environment. We can see from the preceding example that the observation received by the agent is complex, and it would be difficult to train an agent capable of solving this task using the current observation. Let's use environment wrappers to simplify the agent's observation and action space.\n",
    "\n",
    "### Observation Wrapper that Returns Coordinates\n",
    "We would rather like to use the agent's coordinates as observation since this will allow us to use tabular methods to solve this problem. Let's write an observation wrapper that returns the coordinates rather than the field of view of the agent. For this wrapper our class inherits from the `gym.core.ObservationWrapper` i.e.\n",
    "```python\n",
    "class ObservationWrapper(Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        return self.observation(observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        return self.observation(observation), reward, done, info\n",
    "\n",
    "    @abstractmethod\n",
    "    def observation(self, observation):\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "We have to implement the `observation` function. This function transforms the observation before returning it to the agent. We are just going to return the agen't coordinates instead of transforming the current observation.\n",
    "\n",
    "use `self.agent_pos` to get the agent's $x$ and $y$ coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc664e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordsObsWrapper(gym.core.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=np.array([0,0]), \n",
    "                                     high=np.array([env.width-3, env.height-3]),\n",
    "                                     dtype=np.int32) # update the observation space       \n",
    "\n",
    "    def observation(self, observation):\n",
    "        return self.agent_pos[1], self.agent_pos[0] # return the y and x coordinates of the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fbd45",
   "metadata": {},
   "source": [
    "### Action Wrapper\n",
    "\n",
    "The default action space of the environment contains a few actions we do not need to solve the given task. We just need an agent that is able to traverse the grid world. To make things even simple we are going to ommit the actions that turns the agent and just have an agent that can move up, down, left and right. \n",
    "For this wrapper our class inherits from the `gym.core.ActionWrapper` i.e.\n",
    "```python\n",
    "class ActionWrapper(Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self.action(action))\n",
    "\n",
    "    @abstractmethod\n",
    "    def action(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def reverse_action(self, action):\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "We have to implement the `action` function. This function transforms an action before performing it in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourDirectionsActionWrapper(gym.core.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = Discrete(4) # update the action space, the agent can only perform 4 different actions\n",
    "\n",
    "    def action(self, action):\n",
    "        while self.agent_dir != action: # turn the agent until it face the desired direction\n",
    "            self.env.step(0)        \n",
    "        return 2 # move forward in the current direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603716d",
   "metadata": {},
   "source": [
    "### The Reward Function\n",
    "We use a custom reward function in this notebook. The agent receives a reward of +1 if it gets to the green goal. We argued dying is worse than getting to the goal, therefore if the agent falls in the lava it receives a reward of -2. The agent receives an reward of -0.001 for each step it takes in the environment to incentivise it to complete the task as quickly as possible. We've include this wrapper in the notebook, so no need to implement it.\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/reward_function.png\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "Let's apply the wrappers to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b00293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying wrappers to the environment\n",
    "env = RewardWrapper(env) # To receive the above defined rewards\n",
    "env = CoordsObsWrapper(env) # wrapper that returns coordinates as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrapper that transforms action space to a four direction action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431a006",
   "metadata": {},
   "source": [
    "Our new observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b664384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Observation space: {env.observation_space}') # observation space of the environment\n",
    "print(f'Random observation: {env.observation_space.sample()}') # sample random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee588a1",
   "metadata": {},
   "source": [
    "The simplified action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f418b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Action space: {env.action_space}') # action space of the environment\n",
    "print(f'Random action: {env.action_space.sample()}') # sample random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f275d01",
   "metadata": {},
   "source": [
    "### Interacting with the environment\n",
    "\n",
    "The agent in action after applying the wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset() # reset and get an initial observation\n",
    "action = 0\n",
    "a_l = ['⇨','⇩','⇦','⇧']\n",
    "import time\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample() # sample a random action from the action space\n",
    "    obs, reward, done, _ = env.step(action) # perform the action and receive an observation, reward, and if the episode is done\n",
    "    \n",
    "    # render the environment\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_title(f'action: {action} {a_l[action]}, observation: {obs}')\n",
    "    render(env, ax=ax1) # render the environment\n",
    "    \n",
    "    if done: # if the episode has terminated\n",
    "        env.reset() # reset the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9d0a5",
   "metadata": {},
   "source": [
    "## Rewards and Returns\n",
    "\n",
    "The goal of RL agent is described in terms of the reward signal that it receives at every time step.\n",
    "The reward signal is a real number, $R_t \\in \\mathbb{R}$ and the objective of the agent is to maximise the sum of rewards received over time, i.e. the cumulative sum of rewards received in the course of its trajectory and not just the immediate reward.\n",
    "The sum of all the rewards received during a trajectory in MDP is called the *return*, denoted with $G_t$.\n",
    "In the simplest case, the return $G_t$ of a trajectory is the sum of rewards received after the time step $t$,\n",
    "\\begin{equation}\n",
    "    G_t  \\doteq R_{t+1} +  R_{t+2} + R_{t+3} + \\dots+ R_T,\n",
    "        % & = \\sum_{k=0}^{T-1} \\gamma^k R_{t+k+1},\n",
    "\\end{equation}\n",
    "where $T$ is the final time step of the trajectory.\n",
    "We now also discuss the idea of discounting rewards.\n",
    "By using this approach, the agent's objective is to choose actions in order to maximise the expected sum of discounted rewards.\n",
    "Sutton and Barto define the discounted return $G_t$ as\n",
    "\\begin{equation}\\label{eq:returndiscounted}\n",
    "\\begin{aligned}\n",
    "    G_t & \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\\n",
    "    & = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Gamma $\\gamma \\in [0,1]$ is the *discount rate* and determines the value of future rewards by scaling rewards based on the time step when it is acquired.\n",
    "A value close to 0 favours rewards that are acquired in the near future, which leads to a *myopic* evaluation, whereas a value close to 1 assigns equal importance to all future rewards and leads to a more *far-sighted* evaluation.\n",
    "Discounting rewards allows us to favour rewards based on the time they are received.\n",
    "It also allows the return $G_t$ to be finite in tasks where $T=\\infty$.\n",
    "Note that there is a recursive relationship between returns at successive time steps\n",
    "\\begin{equation}\\label{eq:recursivereturn}\n",
    "    \\begin{aligned}\n",
    "    G_t & \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots \\\\\n",
    "    & = R_{t+1} + \\gamma (R_{t+2}+\\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\dots)\\\\\n",
    "    & = R_{t+1} + \\gamma G_{t+1}.\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "This recursive relationship is fundamental to RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48304dd9",
   "metadata": {},
   "source": [
    "## Policies and Value Functions\n",
    "\n",
    "In MDP the agent has to sequentially choose actions to accumulate rewards with the goal to receive the highest possible return.\n",
    "Unfortunately a fixed sequence of actions will not solve MDP due to the uncertainty that is present in the transition model. \n",
    "The agent therefore needs to find a solution where each state in the MDP is mapped to a probability distribution that gives the probability of choosing each possible action.\n",
    "Such a solution is described by Sutton and Barto as a stochastic rule which selects actions as a function of states, and is defined as policy $\\pi$.\n",
    "If the agent is following a policy $\\pi$ at time step $t$, then $\\pi(a|s)$ is the probability of taking the action $a$ given the state $s$.\n",
    "The state-value function $v_\\pi(s)$ is a way to evaluate the quality of a policy.\n",
    "The state-value function $v_\\pi(s)$ is the expected return $G_t$ if the agent is in state $s$ and then follows a policy $\\pi$ and is defined by Sutton and Barto as\n",
    "\\begin{equation}\\label{eq:statevalue}\n",
    "\\begin{aligned}\n",
    "    v_\\pi(s) & \\doteq \\mathbb{E}_\\pi\\left[G_t|S_t = s\\right]\\\\\n",
    "            &= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\middle|S_t = s\\right], \\text{ for all } s \\in \\mathcal{S}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The state-value function $v_\\pi(s)$ is thus an estimate of how good it is to be in a given state.\n",
    "The action-value function $q_\\pi(s,a)$ is another way to evaluate a policy $\\pi$. The action-value function $q_\\pi(s,a)$ is the expected return $G_t$ if the agent is in state $s$, takes action $a$ and then follows policy $\\pi$. Sutton and Barto define it as\n",
    "\\begin{equation}\\label{eq:actionvalue}\n",
    "\\begin{aligned}\n",
    "    q_\\pi(s,a) & \\doteq \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\\\\\n",
    "               &= \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\middle|S_t = s, A_t = a\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The action-value function $q_\\pi(s,a)$ is therefore an estimate of how beneficial it is to perform a specified action in a given state and then following the policy $\\pi$ afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a87e6",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Evaluation\n",
    "\n",
    "How do we determine how good a given policy is? We first discuss how Monte Carlo (MC) methods can be used for prediction or policy evaluation.\n",
    "It has been shown by Sutton and Barto that if the agent follows a policy $\\pi$ and encounters a state $s$, the average return that follows the state $s$  will converge to the state's value $v_\\pi(s)$, as the number of times the state is visited approaches infinity. \n",
    "If an average is tracked for each action taken in the state $s$, then the state-action value $q_\\pi(s,a)$ can similarly be determined.\n",
    "These methods of estimating the value functions are called *Monte Carlo methods* because they entail averaging over many random sample episodes using the actual returns received.\n",
    "As MC methods use samples of entire episodes for policy evaluation, the estimate of the value function is unbiased.\n",
    "The drawback is that MC methods have to wait until the return $G_t$ is known.\n",
    "Therefore values of states can only be updated at the end of an episode.\n",
    "\n",
    "Each time a state $s$ is encountered we refer to it as a *visit* to $s$.\n",
    "The first time the state $s$ is encountered, we refer to as the *first visit* to $s$.\n",
    "There are two main MC policy evaluation methods to estimate $v_\\pi$, namely the *first-visit* MC method and the *every-visit* MC method.\n",
    "It has been shown by Sutton and Barto that the first-visit MC method estimates $v_\\pi(s)$ as the mean return following the first visits to $s$.\n",
    "On the other hand, the every-visit MC method estimates $v_\\pi(s)$ as the mean return following all visits to $s$.\n",
    "According to Sutton and Barto, first-visit MC and every-visit MC converge to $v_\\pi(s)$ as the number of times $s$ is encountered, approaches infinity.\n",
    "The result is an unbiased estimate of the expected value.\n",
    "\n",
    "Now we're going to implement first-visit MC policy evaluation to evaluate a random policy in the gridworld environment. Our agent is going to complete full episodes in the environment and we are going to average the return it receives from each state over all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d02bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.681774Z",
     "start_time": "2021-09-29T07:11:35.671828Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TODO: Rather a class than a function?\n",
    "def MCPE(episodes, env):\n",
    "    returns = np.zeros((env.height, env.width)) # numpy array to keep track of the sum of returns received in state\n",
    "    v_count = np.zeros((env.height, env.width)) # numpy array to keep track of the number of times a state has been visisted\n",
    "    value_function = np.ones((env.height, env.width))*-2 # initialise value function\n",
    "\n",
    "    for i in range(episodes):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        env.seed(1)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done: # complete an episode and store the states and rewards\n",
    "            states.append(state)\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            rewards.append(reward)\n",
    "\n",
    "        G = 0 # set return to zero\n",
    "        for state, reward in zip(states[::-1], rewards[::-1]): # loop over states and rewards in a reverse order\n",
    "            states.pop() # pop the last state from the states list\n",
    "            if state not in states: # check if the state does not appear earlier in the sequence\n",
    "                G += reward # add the reward to the return\n",
    "                v_count[state] += 1 # increment the number of times a first visit was made to this state\n",
    "                returns[state] += G # add G to the total return received from this state\n",
    "\n",
    "        # render plot        \n",
    "        if (i+1) % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(figsize=(8,5))\n",
    "            ax = sns.heatmap(value_function, cmap='RdYlGn', alpha = 0.8, zorder=2, annot = True, fmt='.3f')\n",
    "            img = env.grid.render(tile_size=32)\n",
    "            ax.imshow(img,\n",
    "                      aspect = ax.get_aspect(),\n",
    "                      extent = ax.get_xlim() + ax.get_ylim(),\n",
    "                      interpolation='nearest', zorder = 1)\n",
    "            plt.pause(0.00001)\n",
    "\n",
    "        value_function = np.round(returns/v_count, 3) # calculate the value function averaging the returns received in each state\n",
    "\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.858361Z",
     "start_time": "2021-09-29T07:11:35.683449Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MiniGrid-DistShift1-v0') # create environment\n",
    "env = RewardWrapper(env) # wrap with reward wrapper\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "value_function = MCPE(2000, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00b02f",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "We now know how good it is to follow a random policy from each state in the environment. Next we would like to improve on this policy.\n",
    "Using dynamic programming a model of the environment is required to perform a greedy policy improvement over a state-value function.\n",
    "A greedy policy improvement over $v_\\pi(s)$ is given by\n",
    "\\begin{equation}\\label{eq:statevalueimprovement}\n",
    "    \\pi'(s) = \\underset{a}{\\text{argmax}} \\sum_{s',r}p(s',r|s,a)\\big[r+ \\gamma v_\\pi(s')\\big].\n",
    "\\end{equation}\n",
    "It is clear that the probabilities $p(s',r|s,a)$, which characterise the dynamics of the environment, are required to perform the policy improvement.\n",
    "It is therefore impossible to use the state-value function for policies improvement steps in model-free settings.\n",
    "In these applications, the action-value function is handy as it can be used to improve a policy without having access to the environment's model.\n",
    "A greedy policy improvement over $q_\\pi(s,a)$ is given by\n",
    "\\begin{equation}\n",
    "    \\pi'(s) = \\underset{a}{\\text{argmax }}q_\\pi(s,a).\n",
    "\\end{equation}\n",
    "We therefore use the action-value function in MC control in order to do policy improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2844f",
   "metadata": {},
   "source": [
    "## ε-Greedy Exploration\n",
    "Acting *greedily* with regard to the current estimated action-value function allows us to exploit our current knowledge of the system and allows for the maximisation of the expected reward on the current step.\n",
    "By selecting nongreedy actions, alternative actions are explored and allow for improving the estimates of values of the nongreedy actions.\n",
    "Alternative actions may be better than what current knowledge estimates to be best.\n",
    "After discovering better actions, these can be exploited to produce greater reward in the long run.\n",
    "There is, therefore, a conflict between exploration and exploitation as it is not possible to perform both with a single action.\n",
    "\n",
    "MC methods use samples of experienced episodes to update value functions and to improve policies. \n",
    "Sufficiently exploring all possible state-action pairs with MC methods can be problematic.\n",
    "By selecting only actions that are estimated to be the best, i.e. always performing greedy actions, alternative actions, which may be better, are never selected.\n",
    "We must therefore ensure that the algorithm continues to explore alternative state-action pairs.\n",
    "According to Sutton and Barto we can solve this problem by starting episodes with randomly sampled state-action pairs (with all state-action pairs having a none-zero probability of being sampled).\n",
    "This guarantees that all state-actions pairs will be encountered an infinite number of times for an infinite number of episodes.\n",
    "Sutton and Barto define the above assumption of randomly initialising state-action pairs at the start of an episode as *exploring starts*.\n",
    "In simulated environments, exploring starts can often be implemented, but this method is usually not feasible to use with real-world problems.\n",
    "\n",
    "An alternative way to address the problem of maintaining exploration is to have a stochastic policy that has a non-zero probability of selecting all possible actions in each state.\n",
    "In applications where exploring starts cannot be used, the *$\\varepsilon$-greedy* exploration strategy is convenient to use.\n",
    "It is a very simple, effective exploration strategy that ensures continual exploration.\n",
    "The $\\varepsilon$-greedy strategy is defined as\n",
    "\\begin{equation}\n",
    "    \\pi(a|s) = \\begin{cases} \\varepsilon/|\\mathcal{A}(S_t)| +1-\\varepsilon &\\text{if }a^* = \\underset{a}{\\text{argmax }}Q(s,a) \\\\\n",
    "                        \\varepsilon/|\\mathcal{A}(S_t)| &\\text{otherwise}\n",
    "    \\end{cases},\n",
    "\\end{equation}\n",
    "where $|\\mathcal{A}(S_t)|$ is the cardinality of the action space.\n",
    "Every time the agent performs an action, it has a probability of $1-\\varepsilon$ of selecting the greedy action, else (with a probability of $\\varepsilon$) the action is randomly sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c0339",
   "metadata": {},
   "source": [
    "### Implement ε-greedy exploration\n",
    "\n",
    "Let's implement a simple function that performs $\\varepsilon$-greedy exploration\n",
    "\n",
    "``` python\n",
    "def e_greedy(env, qtable, state, epsilon):\n",
    "    pass\n",
    "```\n",
    "\n",
    "\n",
    "`random.random()` to obtain a random integer between 0 and 1\n",
    "\n",
    "`env.action_space.sample()` to sample a random action from the environment\n",
    "\n",
    "`np.armax(qtable[state])` to retreive the greedy action from the q-table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, qtable, state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(qtable[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3b1eb",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "\n",
    "We are now going to use Monte Carlo estimation to perform control i.e. estimate an optimal policy to solve the task in the environment. We are going to use a concept referred to as generalised policy iteration (GPI). It entails storing an approximate policy and an an approximate value function. The value function is constantly updated to better approximate the current policy. The policy is again updated to be $\\varepsilon$-greedy with respect to the value function. We therefore repeatedly make policy improvements. These two improvement kinds of changes work against each other to some extent, as each creates a moving target for the other, but together they cause both policy and value function to approach optimality.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/monte_carlo_policy_iteration.png\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "It is said that we do not have to precisely evaluate $q_\\pi$, but only to move toward $q_\\pi$. This will still converge to an optimal value function and an optimal policy.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/monte_carlo_control.png\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "We therefore only evaluate $q_\\pi$ for one step at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(episode, eps_end=0.1, eps_start=1, eps_end_episode=300):\n",
    "    return max(eps_end, \n",
    "                ((eps_end - eps_start) / eps_end_episode)*episode \n",
    "                + eps_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "transition = namedtuple('transition', 'state action')\n",
    "\n",
    "def MC_Control_1(episodes, env, epsilon):\n",
    "    q_table = defaultdict(lambda: np.zeros(shape=(env.action_space.n,)))\n",
    "    returns = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        episode = []\n",
    "        rewards = []\n",
    "        env.seed(1)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:            \n",
    "            action = e_greedy(env, q_table, state, get_epsilon(ep))\n",
    "            episode.append((state, action))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            \n",
    "        G = 0\n",
    "        for (state, action), reward in zip(episode[::-1], rewards[::-1]):\n",
    "            episode.pop() # pop the last state from the states list\n",
    "            if (state, action) not in episode: # check if the state-action pair does not appear earlier in the sequence\n",
    "                G += reward\n",
    "                returns[state][action].append(G)\n",
    "                q_table[state][action] = np.mean(returns[state][action])\n",
    "            \n",
    "#         # render\n",
    "#         if ep % 100 == 0:\n",
    "#             clear_output(wait = True)\n",
    "#             fig, ax = plt.subplots()\n",
    "#             plot_q_value_func(ax, q_table, env)\n",
    "#             plt.pause(0.00001)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d53a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "transition = namedtuple('transition', 'state action')\n",
    "\n",
    "def MC_Control_2(episodes, env, epsilon):\n",
    "    q_table = defaultdict(lambda: np.zeros(shape=(env.action_space.n,))) # initialise the q-table\n",
    "    returns = defaultdict(lambda: np.zeros(shape=(env.action_space.n,))) # initialise returns\n",
    "    visits = defaultdict(lambda: np.zeros(shape=(env.action_space.n,))) # initialise visits\n",
    "\n",
    "    # for a number of episodes\n",
    "    for ep in tqdm(range(episodes)): \n",
    "        episode = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done: # perform episode and record states, actions and rewards           \n",
    "            action = e_greedy(env, q_table, state, 0.1)\n",
    "            episode.append((state, action))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            \n",
    "        G = 0 # set return to zero\n",
    "        # loop through episode in a reverse order\n",
    "        for (state, action), reward in zip(episode[::-1], rewards[::-1]):\n",
    "            episode.pop() # pop the last state-action pair from the episode list\n",
    "            if (state, action) not in episode: # check if the state-action pair does not appear earlier in the sequence\n",
    "                G += reward # add reward to the return\n",
    "                returns[state][action] += G # add the return to the total returns received given this state-action pair\n",
    "                visits[state][action] +=1 # increment the first visits to this state-action pair\n",
    "                q_table[state][action] = returns[state][action]/visits[state][action] # update Q-value of state-action pair\n",
    "        \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MiniGrid-Empty-8x8-v0') # create environment\n",
    "env = RewardWrapper(env) # wrap with reward wrapper\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "q_table = MC_Control_2(5000, env, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c9c73",
   "metadata": {},
   "source": [
    "The learned q-table is now demonstrated by performing the greedy action at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f966a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(epsilon, env, qtable):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    for i in range(10):\n",
    "        while not done:\n",
    "            action = e_greedy(env, qtable, state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            render(env)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "demo(epsilon=0, env=env, qtable=q_table) # demonstrate the agent using an epsilon of zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d88d",
   "metadata": {},
   "source": [
    "# Temporal-difference Control\n",
    "\n",
    "Temporal-difference (TD) methods are central to RL and, like MC methods, can learn optimal policies and value functions from environmental interaction without the need for a model that describes the dynamics of the environment.\n",
    "Unlike MC methods, these methods do not have to wait until the end of an episode before updating the value function and improving the policy.\n",
    "They achieve this ability by using bootstrapping.\n",
    "Bootstrapping entails updating estimated values based on other estimated values.\n",
    "This allows them to be more effective in solving continuous tasks or tasks with long episodes, which are problematic for MC methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f79af3",
   "metadata": {},
   "source": [
    "## The Bellman Equation\n",
    "\n",
    "In this section we review the Bellman equation.\n",
    "It can be used to break down the value function to describe the recursive relationship between a state's value and the values of the states that follow it.\n",
    "\n",
    "The recursive relationship between successive returns can be used to decompose the expected return $G_t$ in as the expected immediate reward $R_{t+1}$ plus the expected discounted return at the next time step $\\gamma G_{t+1}$.\n",
    "The state-value function becomes\n",
    "\\begin{equation}\\label{eq:bellmanstatevalue}\n",
    "\\begin{aligned}\n",
    "    v_\\pi(s) & \\doteq \\mathbb{E}_\\pi[G_t|S_t = s]\\\\\n",
    "    &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t = s]\\\\\n",
    "    &= \\sum_a \\pi(a|s) \\sum_{s'}\\sum_r p(s',r|s,a)\\big[r +\\gamma \\mathbb{E}_\\pi[G_{t+1}|S_{t+1} = s']\\big]\\\\\n",
    "    &= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)\\big[r +\\gamma v_\\pi(S_{t+1})\\big], \\text{ for all } s \\in \\mathcal{S}\\\\\n",
    "  &= \\mathbb{E}_\\pi\\big[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t = s\\big].\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The expected discounted return $G_{t+1}$ is replaced with the discounted state-value function of the next state $\\gamma v_\\pi(S_{t+1})$.\n",
    "The outer expectation becomes a summation over the variables $a$, $s'$, and $r$.\n",
    "For each combination of the variables $a$, $s'$, and $r$, the probability $\\pi(a|s)p(s',r|s,a)$ is computed.\n",
    "The value between the brackets is weighted by each probability, then a sum over all possibilities is computed to get an expected value. \n",
    "This is known as the Bellman equation for $v_\\pi$, and is named after Richard Bellman, who developed it in 1957.\n",
    "It states that the value of a given state is equal to the discounted value of the expected next state plus the value of the expected received reward.\n",
    "The action-value function can be similarly decomposed and gives\n",
    "\\begin{equation}\\label{eq:bellmanactionvalue}\n",
    "    q_\\pi(s, a)= \\mathbb{E}_\\pi\\big[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})|S_t = s, A_t = a\\big].\n",
    "\\end{equation}\n",
    "The Bellman equation can be used to check for optimality and for recursive assignments during policy evaluation.\n",
    "\n",
    "Next we are going to implement two TD agents. First we create a base class for these agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd9504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.764720Z",
     "start_time": "2021-09-29T07:12:05.750714Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Base Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name,\n",
    "                 learning_rate=0.1,\n",
    "                 eps_start = 1,\n",
    "                 eps_end= 0.1,\n",
    "                 eps_end_episode = 100,\n",
    "                 episodes=1000,\n",
    "                 gamma=0.95):\n",
    "        \n",
    "        self.env_name = env_name\n",
    "        self.env = self.create_env(env_name)\n",
    "        \n",
    "        # decaying epsilon parameterisation\n",
    "        self.eps_start = eps_start # epsilon start value\n",
    "        self.eps_end = eps_end # epsilon end value\n",
    "        self.eps_end_episode = eps_end_episode # episode to reach end value    \n",
    "        \n",
    "        self.learning_rate = learning_rate # agent learning rate\n",
    "        self.gamma = gamma # discount rate\n",
    "        self.episodes = episodes # number of episode to train agent\n",
    "        self.q_table = defaultdict(lambda: np.zeros(shape=(self.env.action_space.n,))) # initialise q-table\n",
    "        \n",
    "        self.episode_return_history = [] # list to keep track of return received at each episode\n",
    "        self.epsilon_history = [] # list to keep track of epsilon\n",
    "        \n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Returns epsilon given an episode. As the number of episodes increases, the epsilon value decreases.\n",
    "        Epsilon decays until it reaches its specified minimum value.\n",
    "        \"\"\"\n",
    "        return max(self.eps_end, \n",
    "                    ((self.eps_end - self.eps_start) / self.eps_end_episode)*episode \n",
    "                    + self.eps_start)\n",
    "        \n",
    "    def create_env(self, env_name):\n",
    "        \"\"\"\n",
    "        Create and wrap the environment for the agent.\n",
    "        \"\"\"\n",
    "        env = gym.make(env_name)\n",
    "        env = RewardWrapper(env)\n",
    "        env = CoordsObsWrapper(env)\n",
    "        env = FourDirectionsActionWrapper(env)\n",
    "        return env        \n",
    "\n",
    "    def e_greedy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy exploration strategy.\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Method used to train the agent. Each type of agent should implement this.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def log_metrics(self, episode_return, epsilon):\n",
    "        \"\"\"\n",
    "        Method used to log the return and epsilon value at the end of each episode.\n",
    "        \"\"\"\n",
    "        self.episode_return_history.append(episode_return)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Method used to visualise training.\n",
    "        \"\"\"\n",
    "        clear_output(wait = True)   \n",
    "        plt.figure(figsize=(15,10)) \n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        ax3 = plt.subplot(2,1,2)\n",
    "        reward_range = self.env.reward_range\n",
    "        plot_return(ax1, self.episode_return_history, y_min=reward_range[0]*1.1, y_max= reward_range[1]*1.1)\n",
    "        plot_epsilon(ax2, self.epsilon_history)\n",
    "        plot_q_value_func(ax3, self.q_table, self.env)        \n",
    "        plt.pause(0.000001)\n",
    "        \n",
    "    def demo_plot(self, state, pause):\n",
    "        \"\"\"\n",
    "        Method used to visualise a demonstration of the learnt q-table.\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(15,5), gridspec_kw={'width_ratios': [2, 1]}) \n",
    "        img = self.env.render(mode='rgb', highlight=False)\n",
    "        plot_q_value_func(ax1, self.q_table, self.env, alpha = 0.4, img = img)\n",
    "        plt.ylim(-0.1, 1)\n",
    "        \n",
    "        ax2.set_ylabel('Q-Values')\n",
    "        ax2.set_title('Q-Values')\n",
    "        sns.barplot(x=['right', 'down', 'left', 'up'], y=self.q_table[state], ax = ax2)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(pause)\n",
    "\n",
    "    def demo(self, epsilon, episodes=5, pause=0.1, random_start = True):\n",
    "        \"\"\"\n",
    "        Method that demonstrates the learnt policy.\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        self.env.seed(1)\n",
    "        state = self.env.reset(random_start = random_start)\n",
    "        for i in range(episodes):\n",
    "            while not done:\n",
    "                self.demo_plot(state, pause)\n",
    "                action = self.e_greedy(state,epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "            self.env.seed(1)\n",
    "            state = self.env.reset(random_start = random_start)\n",
    "            done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239461ae",
   "metadata": {},
   "source": [
    "## SARSA (state, action, reward, state, action)\n",
    "We now review a SARSA to obtain optimal value functions and optimal policies.\n",
    "According to Sutton and  Barto, the idea of TD learning methods is that the approximate policy and approximate value function can interact with each other in such a way that they both move towards their optimal values.\n",
    "The first part of this process is to estimate the value function to predict the returns of the current policy.\n",
    "The second part is to improve the policy with regard to the estimated value function.\n",
    "\n",
    "In this section, we review an on-policy TD control method, *SARSA*.\n",
    "Again the action-value function $q_\\pi$ is used instead of the state-value function $v_\\pi$, as the former does not require a transition model of the environment to improve the policy.\n",
    "The goal is to obtain an optimal action-value function $q_*$ and therefore also an optimal policy $\\pi_*$.\n",
    "We first discuss how a policy $\\pi$ is evaluated, and then how the policy is improved.\n",
    "\n",
    "Similarly to the previous section, we learn a value function from samples of experience.\n",
    "Here state-action pair to state-action pair is considered, and the estimated values of state-action pairs are updated.\n",
    "The estimated action-value $Q(S_t, A_t)$ is updated:\n",
    "\\begin{equation}\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\big].\n",
    "\\end{equation}\n",
    "The update is done for non-terminal states, and if state $S_{t+1}$ is terminal then $Q(S_{t+1}, A_{t+1})$ is defined as zero.\n",
    "\n",
    "The estimated action-value function $Q$ is then used to improve the policy.\n",
    "An $\\varepsilon$-greedy strategy can be used to choose the action $A_{t+1}$ from the state $S_{t+1}$ using the estimated action-value function $Q$.\n",
    "The $\\varepsilon$-greedy strategy acts mostly greedy with regard to the estimated action-value function but also explores nongreedy actions from time to time.\n",
    "In every update $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$ are present and these give rise to the name SARSA.\n",
    "SARSA for on-policy control since the agent learns about the policy it is following.\n",
    "\n",
    "### Implement the SARSA update step\n",
    "```python\n",
    "def update_sarsa(self, action, state, next_state, next_action, reward, done):\n",
    "    pass\n",
    "```\n",
    "$\\alpha$ -> `self.learning_rate`\n",
    "\n",
    "$\\gamma$ -> `self.gamma`\n",
    "\n",
    "$R_{t+1}$ -> `reward`\n",
    "\n",
    "$Q(S_t, A_t)$ -> `self.q_table[state][action]`\n",
    "\n",
    "$Q(S_{t+1}, A_{t+1})$ -> `self.q_table[next_state][next_action]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b93fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.771560Z",
     "start_time": "2021-09-29T07:12:05.765796Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentSarsa(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_sarsa(self, action, state, next_state, next_action, reward, done):\n",
    "        \"\"\"\n",
    "        SARSA update rule\n",
    "        \"\"\"\n",
    "        # if the episode is done the estimate future return is zero\n",
    "        v_estimate = 0 if done else self.gamma*self.q_table[next_state][next_action]      \n",
    "        # update the q-table using the SARSA update rule\n",
    "        self.q_table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.q_table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        SARSA training loop\n",
    "        \"\"\"\n",
    "        \n",
    "        done = False\n",
    "        state = self.env.reset() # reset the environment and retrieve the initial state\n",
    "        action = self.e_greedy(state, self.get_epsilon(0)) # choose an initial action using e-greedy strategy\n",
    "\n",
    "        episode_return = 0 # set the episode return to zero\n",
    "        \n",
    "        # for a number of episodes\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep) # get the value of epsilon for the current episode\n",
    "            # while the episode is not done\n",
    "            while not done:\n",
    "                # perform action in the environment and get the next state, reward, and if episode is done\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "                episode_return += reward # add reward to the the episode return\n",
    "                if not done: # if not done, choose a next action using the next state\n",
    "                    next_action = self.e_greedy(next_state, epsilon)\n",
    "                \n",
    "                # update q-table using SARSA update rule\n",
    "                self.update_sarsa(action, state, next_state,\n",
    "                                  next_action, reward, done)\n",
    "                state = next_state # set state equal to next state\n",
    "                action = next_action # set action equal to next action\n",
    "            # if episode is done\n",
    "            state = self.env.reset() # reset the environment\n",
    "            action = self.e_greedy(state, epsilon) # choose first action of new episode using e-greedy\n",
    "            done = False # set done to false\n",
    "            \n",
    "            # log metrics and set episode return to zero\n",
    "            self.log_metrics(episode_return, epsilon)\n",
    "            episode_return = 0\n",
    "            \n",
    "            # refresh visualisation every 10 episodes\n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92850f5c",
   "metadata": {},
   "source": [
    "Next we train a SARSA agent using the SARSA update rule you've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde6ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:23:24.055111Z",
     "start_time": "2021-09-29T07:12:05.773670Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa = AgentSarsa(env_name='MiniGrid-DistShift1-v0') # create the SARSA agent\n",
    "agent_sarsa.train() # train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5746ed9",
   "metadata": {},
   "source": [
    "We've added some live plots to help you visualise the agent training.\n",
    "The agent's learning curve is depicted in the upper-left plot. The return received by the agent is plotted against the number of episodes completed in this plot. A higher value indicates a more effective agent. The gray line indicates the actual return received at each episode, while the blue line is a rolling mean with window of 10 episodes.\n",
    "\n",
    "The plot in the upper-right corner depicts the agent's epsilon value versus episodes. We used a decaying epsilon to train the agent.\n",
    "This allows the agent to explore the environment at the beginning of training but to converge on an optimal policy by the end.\n",
    "\n",
    "Finally, we used a heatmap to visualize the Q-values that the agent learned. Each state in the environment is represented by four arrows.\n",
    "Green arrows represent high values, while red arrows represent lower Q-values. Following the dark green arrows, i.e. engaging in greedy behaviour, will complete the task. Let's put our agent's policy to the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca3ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.309684Z",
     "start_time": "2021-09-29T07:23:24.056397Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa.demo(0, pause =0.1, random_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170851bb",
   "metadata": {},
   "source": [
    "The agent is acting greedily toward the Q-value function in this case. The Q-values for the four different actions for each state visited by the agent are shown in the barplot on the right. Our agent chooses the action with the highest value at each timestep. It is worth noting that takes a long path around the lava rather than the shortest path to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdffc2",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "<!-- <p align=\"center\">\n",
    "<img src=\"resources/q-learning.svg\">\n",
    "</p> -->\n",
    "We now discuss second TD method referred to as Q-learning.\n",
    "Q-learning is an off-policy TD method which Sutton and Barto describe as a breakthrough in RL.\n",
    "It is classified as an off-policy method as the behaviour policy is independent of the target policy.\n",
    "The target policy (the policy the agent learns about) is greedy with respect to $Q(s,a)$:\n",
    "\\begin{equation}\n",
    "    \\pi(S_{t+1}) = \\underset{a}{\\text{argmax}} Q(S_{t+1}, a).\n",
    "\\end{equation}\n",
    "The Q-learning target can be simplified to\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    G_t & \\approx R_{t+1} + \\gamma Q\\big(S_{t+1}, \\underset{a}{\\text{argmax}} Q(S_{t+1}, a)\\big)\\\\\n",
    "    &= R_{t+1} + \\gamma \\underset{a}{\\max}  Q(S_{t+1}, a).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The Q-learning update step is defined as\n",
    "\\begin{equation}\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[R_{t+1} + \\gamma \\underset{a}{\\max} Q(S_{t+1}, a) - Q(S_t, A_t) \\big].\n",
    "\\end{equation}\n",
    "The behaviour policy can be anything, but it is usually $\\varepsilon$-greedy with respect to $Q(s,a)$.\n",
    "The behaviour policy can also take the form of experience generated by older policies or by expert demonstrators.\n",
    "The off-policy nature of the algorithm allows it to be more sample-efficient as it can reuse previously generated experience to learn.\n",
    "This also makes it possible to learn from several different policies at the same time.\n",
    "Sutton and Barto state that with Q-learning, the estimate action-value function $Q$ converges to the optimal action-value function $q_*$ when all state-action pairs are infinitely updated.\n",
    "In practice it is not necessary to converge all the way to $q_*$ to obtain an optimal policy $\\pi_*$.\n",
    "\n",
    "### Implement the Q-learning update step\n",
    "Implement the Q-learning update step by using the following parameters.\n",
    "\n",
    "```python\n",
    "def update_q_table(self, action, state, next_state, reward, done):\n",
    "    pass\n",
    "```\n",
    "$\\alpha$ -> `self.learning_rate`\n",
    "\n",
    "$R_{t+1}$ -> `reward`\n",
    "\n",
    "$Q(S_t, A_t)$ -> `self.q_table[state][action]`\n",
    "\n",
    "$\\underset{a}{\\max} Q(S_{t+1}, a)$ -> `np.max(self.q_table[next_state])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01964e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.328505Z",
     "start_time": "2021-09-29T07:24:05.312569Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentQL(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_qlearning(self, action, state, next_state, reward, done):\n",
    "        \"\"\"\n",
    "        Q-learning update rule\n",
    "        \"\"\"\n",
    "        # if the episode is done the estimate future return is zero\n",
    "        v_estimate = 0 if done else self.gamma*np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.q_table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Q-learning training loop\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        episode_return = 0\n",
    "        \n",
    "        # for a number of episodes\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep) # get the value of epsilon for the current episode\n",
    "            # while the episode is not done\n",
    "            while not done:\n",
    "                action = self.e_greedy(state, epsilon) # choose action using e-greedy strategy\n",
    "                # perform action in the environment and get the next state, reward, and if episode is done\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward # add reward to episode return\n",
    "                \n",
    "                # update q-table using Q-learning update rule\n",
    "                self.update_qlearning(action, state, next_state, reward, done)\n",
    "                \n",
    "                state = next_state # set state equal to next state\n",
    "                \n",
    "            # if episode is done\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # log metrics and set episode return to zero\n",
    "            self.log_metrics(episode_return, epsilon)\n",
    "            episode_return = 0\n",
    "            \n",
    "            # refresh visualisation every 10 episodes\n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75451a48",
   "metadata": {},
   "source": [
    "Next we train an Q-learning agent using the update step you've implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeb547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.614861Z",
     "start_time": "2021-09-29T07:24:05.330277Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q = AgentQL(env_name='MiniGrid-DistShift1-v0') # initialise the q-learning agent\n",
    "agent_q.train() # train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0ab59",
   "metadata": {},
   "source": [
    "The same visualizations are used to monitor training. Take note of how the learning curve and Q-table heatmap differ from those of the SARSA agent. What's the reason for this?\n",
    "\n",
    "Following that, we demonstrated the policy that our Q-learning agent had learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c7042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.618202Z",
     "start_time": "2021-09-29T07:34:51.615936Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q.demo(0,pause=0.05, episodes=10, random_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ff4ff",
   "metadata": {},
   "source": [
    "Compare the policy of the Q-learning agent with the one of the SARSA agent. How do they differ and what is the reason the difference? How can we train a SARSA agent that behaves more like the Q-learning agent?\n",
    "\n",
    "Let's plot the distribution of returns received by the SARSA and Q-learning agent's during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a80748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.725028Z",
     "start_time": "2021-09-29T07:34:51.619733Z"
    }
   },
   "outputs": [],
   "source": [
    "dis_df = pd.DataFrame({'SARSA': agent_sarsa.episode_return_history, 'Q-Learning': agent_q.episode_return_history})\n",
    "sns.displot(dis_df, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4f2e8",
   "metadata": {},
   "source": [
    "From the above plot it seems that the SARSA agent performs better during training. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81904ea",
   "metadata": {},
   "source": [
    "Finally, we can apply this agent to some of the other environments as well. Some of the environments initialise differently each time they are reset. For these environments just call `env.seed(1)` to train in the exact same environment each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_q = AgentQL(env_name='MiniGrid-DistShift2-v0')\n",
    "agent_q.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
