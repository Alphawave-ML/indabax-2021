{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6beecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ByteFuse/indabax-2021.git\n",
    "!cp indabax-2021/llama-challenge/* . -r\n",
    "\n",
    "!pip install librosa==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, just go to the link below and download the data there\n",
    "# https://drive.google.com/drive/folders/1ZucIf6bk0HXQt2mqpY_HPAHWP9UhUg6s?usp=sharing\n",
    "# the csv is downloaded with the git clone above\n",
    "\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1ST6aZkNRH_iUbYjjOnXjpk3uRw5nHo7g'\n",
    "output = 'data.zip'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ca4d4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830e7a2",
   "metadata": {},
   "source": [
    "Welcome to the ByteFuse AI Llama challenge. These challenges differ from others in that we want you to focus on what you can do with models rather than on model design and training.\n",
    "\n",
    "\n",
    "\n",
    "To understand the goal of this challenge, put yourself in the shoes of one of the engineers at ByteFuse. You arrive at work one morning to learn that one of the research scientists has begun work on a system capable of performing multimodal search over both images and speech. However, the scientist only trained the system partially on the [Flickr Audio Caption Corpus](https://groups.csail.mit.edu/sls/downloads/flickraudio/) because the research team is now focusing on finishing their submissions for a conference. This means that the current models aren't ideal, but if you search for a dog, you'll get images of dogs, but fine-grained detail will be missed.\n",
    "\n",
    "\n",
    "The research scientist approaches you and asks if you can create a proof of concept (POC) for a product using the system to demonstrate to everyone the importance of the system, while also allowing them to find out what should be changed in the training and design process for the models to work. The only constraint for the product, according to the scientist, is that it performs multimodal search, either speech->image, image->speech, or both directions. Despite the scientist's belief that the provided models will be a good starting point, the product does not have to be built using the provided models. You can either fine-tune the existing models or create entirely new model architectures and train them from scratch.\n",
    "\n",
    "This is a common scenario at ByteFuse, where we are tasked with finding practical applications for machine learning systems before devoting all of our time and resources to the machine learning system. We want you to feel this rush right now. Take advantage of our models (or don't) and impress us with your work! We will not be judging how well the machine learning component works, but rather how novel the idea is and how well your product works as a concept.\n",
    "\n",
    "This notebook will serve as a guide on how to use some of the utility functions provided. If you want inspiration on methods that you could use to fine these models, refer to our introduction notebook on multimodal search [here](https://colab.research.google.com/github/ByteFuse/indabax-2021/blob/main/multimodal-search/bytefuse_indabax_multimodal_search_intro.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af737b8a",
   "metadata": {},
   "source": [
    "# Setting up the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dcdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "# if you are in the same directory as the utils file\n",
    "from utils import process_audio, process_image, return_pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b53467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_subplots(data):\n",
    "    \n",
    "  plt.figure(figsize=(10,10))\n",
    "\n",
    "  plt.subplot(2, 2, 1)\n",
    "  plt.imshow(data[0], cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "  plt.subplot(2, 2, 2)\n",
    "  plt.imshow(data[1], cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "  plt.subplot(2, 2, 3)\n",
    "  plt.imshow(data[2], cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "  plt.subplot(2, 2, 4)\n",
    "  plt.imshow(data[3], cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208c389",
   "metadata": {},
   "source": [
    "# Loading in our pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb276b4",
   "metadata": {},
   "source": [
    "As mentioned above, our research scientist has managed to train a speech and image model, to a relatively ok state. This means that a lot of improvement can be made on them, but it is not nescarry to only build a POC to show the idea for your product. \n",
    "\n",
    "But remember, if you want to improve on them then go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models with the utility function\n",
    "audio_model, image_model = return_pretrained_models(root_path_to_weights='./model_weights/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73035fda",
   "metadata": {},
   "source": [
    "# Creating a mini database of flickr audio and image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b42bf",
   "metadata": {},
   "source": [
    "Here we will sample from data from the flickr dataset and generate representations for them, and add it to our \"database\" that we can use when we build a search engine.\n",
    "\n",
    "\n",
    "Below we read in our data and show what was used for training, and what was used for validation and testing during the research scientists model training phase. For our database, we will take all the validation and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ccd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have downloaded the data using the top cell of the notebook to your current folder\n",
    "df = pd.read_csv('./bytefuse_challenge_flickr_meta.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.split.isin(['test'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff280f06",
   "metadata": {},
   "source": [
    "Of course the below can be sped up by batching you data and some few other trick, but that is left for you to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = df.audio_file.values\n",
    "\n",
    "representations = []\n",
    "for audio_file in tqdm(audio_files):\n",
    "    audio = process_audio(audio_path=f'./flickr_audio_8k/{audio_file[:-6]}/{audio_file}', original_sample_rate=16e3)\n",
    "    with torch.no_grad():\n",
    "        representations.append(audio_model(audio.unsqueeze(0))[0].numpy())\n",
    "        \n",
    "audio_embeddings = torch.tensor(representations)\n",
    "audio_embeddings = F.normalize(audio_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = df.drop_duplicates('image_file').image_file.values\n",
    "\n",
    "\n",
    "representations = []\n",
    "for image_file in tqdm(image_files):\n",
    "    image = process_image(image_path=f'./flickr_audio_8k/{image_file[:-4]}/{image_file}')\n",
    "    with torch.no_grad():\n",
    "        representations.append(image_model(image.unsqueeze(0))[0].numpy())\n",
    "        \n",
    "image_embeddings = torch.tensor(representations)\n",
    "image_embeddings = F.normalize(image_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684c268",
   "metadata": {},
   "source": [
    "# Doing searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3cf31",
   "metadata": {},
   "source": [
    "The models were trained to find similar points using cosine similarity. Because we normalized our database in the code above, we can speed up big calculations over our database by doing matrix multiplications when we have a target embedding. \n",
    "\n",
    "This is shown in the code below, were we first read in a image and recording from the flickr dataset, then we perform image->speech and speech->image search. You will notice that these partially trained models seem to do better in the image->speech search. Could you improve the speech->image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba81fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './flickr_audio_8k/'\n",
    "id_ = '3335773346_ac0d97efeb'\n",
    "\n",
    "image_path = os.path.join(root, id_,f'{id_}.jpg')\n",
    "audio_path = os.path.join(root, id_,f'{id_}_0.wav') # you can choose between [0,1,2,3,4]\n",
    "\n",
    "\n",
    "image = torchvision.io.read_image(image_path)\n",
    "plt.title('Target image')\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "\n",
    "print('Target speech')\n",
    "audio_, sr = librosa.load(audio_path, 16e3) \n",
    "ipd.display(ipd.Audio(audio_, rate=16e3))\n",
    "\n",
    "image = process_image(image_path=os.path.join(image_path))\n",
    "audio = process_audio(audio_path=os.path.join(audio_path), original_sample_rate=16e3)\n",
    "                      \n",
    "with torch.no_grad():\n",
    "    target_image_embedding = image_model(image.unsqueeze(0))\n",
    "    target_image_embedding = F.normalize(target_image_embedding, p=2, dim=1)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    target_audio_embedding = audio_model(audio.unsqueeze(0))\n",
    "    target_audio_embedding = F.normalize(target_audio_embedding, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1998d7f",
   "metadata": {},
   "source": [
    "Below we do speech->image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(loc):\n",
    "    image = torchvision.io.read_image(f'./flickr_audio_8k/{loc[:-4]}/{loc}')\n",
    "    return image\n",
    "\n",
    "with torch.no_grad():\n",
    "    dist = (image_embeddings @ target_audio_embedding.T)\n",
    "\n",
    "index_sorted = torch.argsort(torch.abs(dist).squeeze(-1))\n",
    "top_5 = index_sorted[-5:]\n",
    "bottom_5 = index_sorted[:5]\n",
    "\n",
    "print('Closest images')\n",
    "draw_subplots([get_image(image_files[i]).permute(1,2,0) for i in top_5])\n",
    "\n",
    "print('Farthest images')\n",
    "draw_subplots([get_image(image_files[i]).permute(1,2,0) for i in bottom_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f76bb",
   "metadata": {},
   "source": [
    "Below we do image->speech search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a860f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dist = (audio_embeddings @ target_image_embedding.T)\n",
    "        \n",
    "index_sorted = torch.argsort(dist.squeeze(-1))\n",
    "top_5 = index_sorted[-5:]\n",
    "bottom_5 = index_sorted[:5]\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "print('3 Closest audio')\n",
    "\n",
    "for i in range(3):\n",
    "    audio_file = audio_files[top_5[i]]\n",
    "    audio_, sr = librosa.load(f'./flickr_audio_8k/{audio_file[:-6]}/{audio_file}', 16e3)\n",
    "    ipd.display(ipd.Audio(audio_, rate=sr))\n",
    "    print()\n",
    "\n",
    "\n",
    "print('3 most different audio')\n",
    "for i in range(3):\n",
    "    audio_file = audio_files[bottom_5[i]]\n",
    "    audio_, sr = librosa.load(f'./flickr_audio_8k/{audio_file[:-6]}/{audio_file}', 16e3)\n",
    "    ipd.display(ipd.Audio(audio_, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab44ac",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9c02f",
   "metadata": {},
   "source": [
    "This should be enough now to get you go going. Remember, ANYTHING is allowed. Search just means finding similar things, so how can you then use that concept to build a POC.\n",
    "\n",
    "We look forward to what you bring us!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2273af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indabax",
   "language": "python",
   "name": "indabax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
