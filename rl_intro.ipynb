{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564f41e-fb34-4e8b-90c9-291f61d21543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.557719Z",
     "start_time": "2021-09-29T07:11:27.823599Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import gym\n",
    "import pandas as pd\n",
    "import gym_minigrid\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "from random import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from minigrid_wrappers import CoordsObsWrapper, FourDirectionsActionWrapper, RewardWrapper\n",
    "from utils import render, draw_square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90622048",
   "metadata": {},
   "source": [
    "# Getting Started with OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005035c",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87e2c3",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "`action space`\n",
    "\n",
    "`observation space`\n",
    "\n",
    "### Methods\n",
    "`def step`\n",
    "\n",
    "`def reset`\n",
    "\n",
    "`def render`\n",
    "\n",
    "### Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc47c1",
   "metadata": {},
   "source": [
    "## Gym Minigrid \n",
    "https://github.com/maximecb/gym-minigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ddadc",
   "metadata": {},
   "source": [
    "## Example Minigrid Environments\n",
    "<ol>\n",
    "    <li>MiniGrid-Empty-8x8-v0</li>\n",
    "    <li>MiniGrid-DistShift1-v0</li>\n",
    "    <li>MiniGrid-DistShift2-v0</li>\n",
    "    <li>MiniGrid-LavaCrossingS9N3-v0</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615d356",
   "metadata": {},
   "source": [
    "Actions in the basic Minigrid environment:\n",
    "<ol>\n",
    "<li>Turn left</li>\n",
    "<li>Turn right</li>\n",
    "<li>Move forward</li>\n",
    "<li>Pick up an object</li>\n",
    "<li>Drop the object being carried</li>\n",
    "<li>Toggle (open doors, interact with objects)</li>\n",
    "<li>Done (task completed, optional)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc3930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.561365Z",
     "start_time": "2021-09-29T07:11:28.558898Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-DistShift1-v0\") # create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ddf97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.569779Z",
     "start_time": "2021-09-29T07:11:28.562702Z"
    }
   },
   "outputs": [],
   "source": [
    "env.action_space # action space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd605e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.576059Z",
     "start_time": "2021-09-29T07:11:28.570750Z"
    }
   },
   "outputs": [],
   "source": [
    "env.observation_space # observation space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2e9ed-6de2-42e1-a92a-3064fb257d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.670151Z",
     "start_time": "2021-09-29T07:11:28.576988Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "obs = env.reset() # reset and get an initial observation\n",
    "\n",
    "action = 0\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample() # sample a random action from the action space\n",
    "    obs, reward, done, _ = env.step(action) # perform the action and receive an observation, reward, and if the episode is done\n",
    "    render(env) # render the environment\n",
    "    if done: # if the episode has terminated\n",
    "        env.reset() # reset the environment\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8b6e2",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a87e6",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Evaluation\n",
    "\n",
    "TODO: explain monte carlo policy evaluation.\n",
    "add notes, pictures, equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d02bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.681774Z",
     "start_time": "2021-09-29T07:11:35.671828Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def MCPE(episodes, env):\n",
    "    returns = np.zeros((env.height, env.width))\n",
    "    v_count = np.zeros((env.height, env.width))\n",
    "    value_function = np.ones((env.height, env.width))*-2\n",
    "\n",
    "    for i in tqdm(range(episodes), ncols=0):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        states.append(state)\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        G = 0\n",
    "        for state, reward in zip(states[:-1], rewards[::-1]):\n",
    "            G += reward\n",
    "            v_count[state] += 1\n",
    "            returns[state] += G\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            ax = sns.heatmap(value_function, cmap='RdYlGn', linewidth=1)\n",
    "            plt.pause(0.00001)\n",
    "\n",
    "        value_function = np.round(returns/v_count, 3)\n",
    "\n",
    "    return np.round(returns/v_count, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.858361Z",
     "start_time": "2021-09-29T07:11:35.683449Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-DistShift1-v0\") # create environment\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9946778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.745925Z",
     "start_time": "2021-09-29T07:11:35.860133Z"
    }
   },
   "outputs": [],
   "source": [
    "value_function = MCPE(1000, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd612c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.749806Z",
     "start_time": "2021-09-29T07:12:05.747044Z"
    }
   },
   "outputs": [],
   "source": [
    "print(value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9226e0b",
   "metadata": {},
   "source": [
    "## Temporal Difference Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d88d",
   "metadata": {},
   "source": [
    "# Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd9504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.764720Z",
     "start_time": "2021-09-29T07:12:05.750714Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "normalize_qs = Normalize(vmin = -1, vmax = 1)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Base Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name,\n",
    "                 learning_rate=0.1,\n",
    "                 eps_start = 1,\n",
    "                 eps_end= 0.1,\n",
    "                 eps_end_episode = 500,\n",
    "                 episodes=1000,\n",
    "                 gamma=0.95):\n",
    "        self.action_space = ['right', 'down', 'left', 'up']\n",
    "        self.env_name = env_name\n",
    "        self.env = self.create_env(env_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mode = 'rgb'\n",
    "        self.size = 32\n",
    "        \n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_end_episode = eps_end_episode        \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        num_actions = self.env.action_space.n\n",
    "        self.table = defaultdict(lambda: np.zeros(shape=(num_actions,)))\n",
    "        \n",
    "        self.episode_return_history = []\n",
    "        self.epsilon_history = []\n",
    "        \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.eps_end, self.eps_start\n",
    "              - (episode + 1) / self.eps_end_episode)\n",
    "        \n",
    "    def create_env(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        env = RewardWrapper(env)\n",
    "        env = CoordsObsWrapper(env)\n",
    "        env = FourDirectionsActionWrapper(env)\n",
    "        return env        \n",
    "\n",
    "    def e_greedy(self, state, epsilon):\n",
    "        if random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.table[state])\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def plot(self):\n",
    "#         print(self.table)\n",
    "        clear_output(wait = True)   \n",
    "        plt.figure(figsize=(15,10)) \n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        ax3 = plt.subplot(2,1,2)\n",
    "        ax1.set_xlabel('Episodes')\n",
    "        ax1.set_ylabel('Return')\n",
    "        ax1.set_title('Return versus Episodes')\n",
    "        ax1.set_ylim(-2.5,1.5)\n",
    "        ax1.plot(np.array(self.episode_return_history), linestyle='dashed', color='silver', lw=1.5)\n",
    "        ax1.plot(pd.Series(self.episode_return_history).rolling(window=10).mean().values, lw=2)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_xlabel('Episodes')\n",
    "        ax2.set_ylabel('Epsilon')\n",
    "        ax2.set_title('Epsilon versus Episodes')\n",
    "        ax2.plot(self.epsilon_history)\n",
    "\n",
    "        df_table = pd.DataFrame(self.table).T.reset_index()\n",
    "        df_table.columns = ['y', 'x', 'right', 'down', 'left', 'up']\n",
    "        df_table = df_table.sort_values(by=['y','x'])\n",
    "        df_table['lb_x'] = df_table['x']*32\n",
    "        df_table['lb_y'] = df_table['y']*32\n",
    "        \n",
    "        df_table[['right', 'down', 'left', 'up']] = normalize_qs(df_table[['right', 'down', 'left', 'up']])\n",
    "        \n",
    "        img = self.env.grid.render(tile_size=32)\n",
    "        ax3.imshow(img, interpolation='nearest')\n",
    "        for row in df_table.itertuples():\n",
    "            lb_y = row[-1]\n",
    "            lb_x = row[-2]\n",
    "\n",
    "            right = row[3]\n",
    "            down = row[4]\n",
    "            left = row[5]\n",
    "            up = row[6]\n",
    "                \n",
    "            draw_square(lb_x, \n",
    "                        lb_y, \n",
    "                        self.size, \n",
    "                        alpha=1, \n",
    "                        r_color=(right),\n",
    "                        b_color=(down),\n",
    "                        l_color=(left),\n",
    "                        t_color=(up))\n",
    "        \n",
    "        plt.pause(0.000001)\n",
    "        \n",
    "    def demo_plot(self, state):\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(15,5), gridspec_kw={'width_ratios': [2, 1]}) \n",
    "        img = self.env.render(mode='rgb')\n",
    "        ax1.imshow(img, interpolation='nearest')\n",
    "        plt.ylim(-0.1, 1)\n",
    "        \n",
    "        ax2.set_ylabel('Q-Values')\n",
    "        ax2.set_title('Q-Values')\n",
    "        sns.barplot(x=['right', 'down', 'left', 'up'], y=self.table[state], ax = ax2)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.5)\n",
    "\n",
    "    def demo(self, epsilon):\n",
    "        print('Demo')\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            while not done:\n",
    "                self.demo_plot(state)\n",
    "                action = self.e_greedy(state,epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "            state = self.env.reset()\n",
    "            done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239461ae",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "Training a SARSA agent\n",
    "\n",
    "TODO: add some notes, pictures, equations of SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b93fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.771560Z",
     "start_time": "2021-09-29T07:12:05.765796Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentSarsa(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_sarsa(self, action, state, next_state, next_action, reward, done):\n",
    "        v_estimate = 0 if done else self.gamma*self.table[next_state][next_action]\n",
    "        \n",
    "        self.table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        action = self.e_greedy(state, self.get_epsilon(0))\n",
    "\n",
    "        episode_return = 0\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep)\n",
    "            while not done:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                if not done:\n",
    "                    next_action = self.e_greedy(next_state, epsilon)\n",
    "                self.update_sarsa(action, state, next_state,\n",
    "                                  next_action, reward, done)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            state = self.env.reset()\n",
    "            action = self.e_greedy(state, epsilon)\n",
    "            done = False\n",
    "            \n",
    "            self.episode_return_history.append(episode_return)\n",
    "            episode_return = 0\n",
    "            self.epsilon_history.append(epsilon)\n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9768eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:23:24.055111Z",
     "start_time": "2021-09-29T07:12:05.773670Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa = AgentSarsa(env_name='MiniGrid-DistShift1-v0') #\n",
    "agent_sarsa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca3ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.309684Z",
     "start_time": "2021-09-29T07:23:24.056397Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa.demo(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70356c",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Training a Q-learning agent\n",
    "\n",
    "//TODO:\n",
    "add some notes and theory about Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01964e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.328505Z",
     "start_time": "2021-09-29T07:24:05.312569Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentQL(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_q_table(self, action, state, next_state, reward, done):\n",
    "        v_estimate = 0 if done else self.gamma*np.max(self.table[next_state])\n",
    "        self.table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        step = 0\n",
    "        episode_return = 0\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep)\n",
    "            while not done:\n",
    "                action = self.e_greedy(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                self.update_q_table(action, state, next_state, reward, done)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            self.episode_return_history.append(episode_return)\n",
    "            episode_return = 0\n",
    "            self.epsilon_history.append(epsilon)\n",
    "            \n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeb547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.614861Z",
     "start_time": "2021-09-29T07:24:05.330277Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q = AgentQL(env_name='MiniGrid-DistShift1-v0')\n",
    "agent_q.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c7042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.618202Z",
     "start_time": "2021-09-29T07:34:51.615936Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q.demo(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a80748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.725028Z",
     "start_time": "2021-09-29T07:34:51.619733Z"
    }
   },
   "outputs": [],
   "source": [
    "dis_df = pd.DataFrame({'SARSA': agent_sarsa.episode_return_history, 'Q-Learning': agent_q.episode_return_history})\n",
    "sns.displot(dis_df, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11c711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.728810Z",
     "start_time": "2021-09-29T07:34:51.728800Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.demo(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
