{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564f41e-fb34-4e8b-90c9-291f61d21543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.557719Z",
     "start_time": "2021-09-29T07:11:27.823599Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import gym\n",
    "import pandas as pd\n",
    "import gym_minigrid\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "from random import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from minigrid_wrappers import CoordsObsWrapper, FourDirectionsActionWrapper, RewardWrapper\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "normalize_qs = Normalize(vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bfd55",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def render(env, mode='rgb'):\n",
    "    img = env.render(mode=mode, highlight=False)\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    clear_output(wait=True)\n",
    "    plt.pause(0.00001)\n",
    "\n",
    "\n",
    "def draw_triangle(ax, p1=[1, 1], p2=[2, 2.5], p3=[3, 1], c='blue', alpha=0.3):\n",
    "    X = np.array([p1, p2, p3])\n",
    "    Y = [c, c, c]\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=0.001, color=Y[:])\n",
    "    t1 = plt.Polygon(X[:3, :], color=Y[0], alpha=alpha)\n",
    "    ax.add_patch(t1)\n",
    "\n",
    "\n",
    "def draw_square(ax,\n",
    "                lb_x,\n",
    "                lb_y,\n",
    "                size,\n",
    "                alpha=1,\n",
    "                l_color='green',  # left\n",
    "                r_color='yellow',  # right\n",
    "                b_color='red',  # down\n",
    "                t_color='blue'):  # up\n",
    "    tr_x = lb_x+size  # top right x\n",
    "    tr_y = lb_y+size  # top right y\n",
    "\n",
    "    md_x = lb_x + size/2  # middle x\n",
    "    md_y = lb_y + size/2  # middle x\n",
    "\n",
    "    width = 10\n",
    "\n",
    "    md_x_plus = md_x + width/2\n",
    "    md_x_minus = md_x - width/2\n",
    "\n",
    "    md_y_plus = md_y + width/2\n",
    "    md_y_minus = md_y - width/2\n",
    "\n",
    "    colormap = cm.get_cmap('RdYlGn')\n",
    "\n",
    "    color_right = colormap(r_color)\n",
    "    color_down = colormap(b_color)\n",
    "    color_left = colormap(l_color)\n",
    "    color_up = colormap(t_color)\n",
    "\n",
    "    draw_triangle(ax,[lb_x+2, md_y], [md_x_minus, md_y_minus+2],\n",
    "                  [md_x_minus, md_y_plus-2], c=color_left, alpha=alpha)  # left\n",
    "    draw_triangle(ax,[tr_x-2, md_y], [md_x_plus, md_y_minus+2],\n",
    "                  [md_x_plus, md_y_plus-2], c=color_right, alpha=alpha)  # right\n",
    "    draw_triangle(ax,[md_x, tr_y-2], [md_x_minus+2, md_y_plus],\n",
    "                  [md_x_plus-2, md_y_plus], c=color_down, alpha=alpha)  # down\n",
    "    draw_triangle(ax,[md_x, lb_y+2], [md_x_minus+2, md_y_minus],\n",
    "                  [md_x_plus-2, md_y_minus], c=color_up, alpha=alpha)  # up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65df484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_return(ax, returns):\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Return')\n",
    "    ax.set_title('Return versus Episodes')\n",
    "    ax.set_ylim(-2.5,1.5)\n",
    "    ax.plot(np.array(returns), linestyle='dashed', color='silver', lw=1.5)\n",
    "    ax.plot(pd.Series(returns).rolling(window=10).mean().values, lw=2)\n",
    "\n",
    "def plot_epsilon(ax, epsilon_history):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Epsilon')\n",
    "    ax.set_title('Epsilon versus Episodes')\n",
    "    ax.plot(epsilon_history)\n",
    "\n",
    "def plot_q_value_func(ax, table, env, alpha = 1, img = None):\n",
    "    df_table = pd.DataFrame(table).T.reset_index()\n",
    "    df_table.columns = ['y', 'x', 'right', 'down', 'left', 'up']\n",
    "    df_table = df_table.sort_values(by=['y','x'])\n",
    "    df_table['lb_x'] = df_table['x']*32\n",
    "    df_table['lb_y'] = df_table['y']*32\n",
    "\n",
    "    df_table[['right', 'down', 'left', 'up']] = normalize_qs(df_table[['right', 'down', 'left', 'up']])\n",
    "    if img is None:\n",
    "        img = env.grid.render(tile_size=32)\n",
    "    ax.imshow(img, interpolation='nearest')\n",
    "    for row in df_table.itertuples():             \n",
    "        draw_square(ax,\n",
    "                    row[-2], # lb_x\n",
    "                    row[-1], # lb_y\n",
    "                    32, \n",
    "                    alpha=alpha, \n",
    "                    r_color=row[3], # right\n",
    "                    b_color=row[4], # down\n",
    "                    l_color=row[5], # left\n",
    "                    t_color=row[6]) # up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90622048",
   "metadata": {},
   "source": [
    "# Getting Started with OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005035c",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87e2c3",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "`action space`\n",
    "\n",
    "`observation space`\n",
    "\n",
    "### Methods\n",
    "`def step`\n",
    "\n",
    "`def reset`\n",
    "\n",
    "`def render`\n",
    "\n",
    "### Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc47c1",
   "metadata": {},
   "source": [
    "## Gym Minigrid \n",
    "https://github.com/maximecb/gym-minigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ddadc",
   "metadata": {},
   "source": [
    "## Example Minigrid Environments\n",
    "<ol>\n",
    "    <li>MiniGrid-Empty-8x8-v0</li>\n",
    "    <li>MiniGrid-DistShift1-v0</li>\n",
    "    <li>MiniGrid-DistShift2-v0</li>\n",
    "    <li>MiniGrid-LavaCrossingS9N3-v0</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615d356",
   "metadata": {},
   "source": [
    "Actions in the basic Minigrid environment:\n",
    "<ol>\n",
    "<li>Turn left</li>\n",
    "<li>Turn right</li>\n",
    "<li>Move forward</li>\n",
    "<li>Pick up an object</li>\n",
    "<li>Drop the object being carried</li>\n",
    "<li>Toggle (open doors, interact with objects)</li>\n",
    "<li>Done (task completed, optional)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc3930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.561365Z",
     "start_time": "2021-09-29T07:11:28.558898Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-DistShift1-v0\") # create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ddf97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.569779Z",
     "start_time": "2021-09-29T07:11:28.562702Z"
    }
   },
   "outputs": [],
   "source": [
    "env.action_space # action space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd605e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:28.576059Z",
     "start_time": "2021-09-29T07:11:28.570750Z"
    }
   },
   "outputs": [],
   "source": [
    "env.observation_space # observation space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b00293",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RewardWrapper(env)\n",
    "env = CoordsObsWrapper(env)\n",
    "env = FourDirectionsActionWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2e9ed-6de2-42e1-a92a-3064fb257d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.670151Z",
     "start_time": "2021-09-29T07:11:28.576988Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "obs = env.reset() # reset and get an initial observation\n",
    "\n",
    "action = 0\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample() # sample a random action from the action space\n",
    "    obs, reward, done, _ = env.step(action) # perform the action and receive an observation, reward, and if the episode is done\n",
    "    render(env) # render the environment\n",
    "    if done: # if the episode has terminated\n",
    "        env.reset() # reset the environment\n",
    "#     time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8b6e2",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a87e6",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Evaluation\n",
    "\n",
    "TODO: explain monte carlo policy evaluation.\n",
    "add notes, pictures, equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d02bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.681774Z",
     "start_time": "2021-09-29T07:11:35.671828Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def MCPE(episodes, env):\n",
    "    returns = np.zeros((env.height, env.width))\n",
    "    v_count = np.zeros((env.height, env.width))\n",
    "    value_function = np.ones((env.height, env.width))*-2\n",
    "\n",
    "    for i in range(episodes):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            states.append(state)\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            rewards.append(reward)\n",
    "\n",
    "        G = 0\n",
    "        for state, reward in zip(states[::-1], rewards[::-1]):\n",
    "            G += reward\n",
    "            v_count[state] += 1\n",
    "            returns[state] += G\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(figsize=(8,5))\n",
    "            ax = sns.heatmap(value_function, cmap='RdYlGn', alpha = 0.8, zorder=2, annot = True, fmt='.3f')\n",
    "            img = env.grid.render(tile_size=32)\n",
    "            ax.imshow(img,\n",
    "                      aspect = ax.get_aspect(),\n",
    "                      extent = ax.get_xlim() + ax.get_ylim(),\n",
    "                      interpolation='nearest', zorder = 1)\n",
    "            plt.pause(0.00001)\n",
    "\n",
    "        value_function = np.round(returns/v_count, 3)\n",
    "\n",
    "    return np.round(returns/v_count, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:11:35.858361Z",
     "start_time": "2021-09-29T07:11:35.683449Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-DistShift2-v0\") # create environment\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "value_function = MCPE(2000, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2844f",
   "metadata": {},
   "source": [
    "## Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, qtable, state, epsilon):\n",
    "    if random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(qtable[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3b1eb",
   "metadata": {},
   "source": [
    "## Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "transition = namedtuple('transition', 'state action')\n",
    "\n",
    "def MC_Control(episodes, env):\n",
    "    q_table = defaultdict(lambda: np.zeros(shape=(env.action_space.n,)))\n",
    "    returns = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        episode = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:            \n",
    "            action = e_greedy(env, q_table, state, 0.1)\n",
    "            episode.append((state, action))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        G = 0\n",
    "        for (state, action), reward in zip(episode[::-1], rewards[::-1]):\n",
    "            G += reward\n",
    "            returns[state][action].append(G)\n",
    "            q_table[state][action] = np.mean(returns[state][action])\n",
    "        if ep % 100 == 0:\n",
    "            clear_output(wait = True)\n",
    "            fig, ax = plt.subplots()\n",
    "            plot_q_value_func(ax, q_table, env)\n",
    "            plt.pause(0.00001)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-DistShift2-v0\") # create environment\n",
    "env = CoordsObsWrapper(env) # wrap to get coordinates of agent as observation\n",
    "env = FourDirectionsActionWrapper(env) # wrap to simplify action space (4 directional movement)\n",
    "q_table = MC_Control(2000, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(epsilon, env, qtable):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    for i in range(5):\n",
    "        print(i)\n",
    "        while not done:\n",
    "            action = e_greedy(env, qtable, state,epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            render(env)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "demo(0, env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [1,2,3,4,5,6]\n",
    "print(mylist)\n",
    "print(mylist[:-1])\n",
    "print(mylist[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9226e0b",
   "metadata": {},
   "source": [
    "## Temporal Difference Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d88d",
   "metadata": {},
   "source": [
    "# Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd9504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.764720Z",
     "start_time": "2021-09-29T07:12:05.750714Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Base Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name,\n",
    "                 learning_rate=0.1,\n",
    "                 eps_start = 1,\n",
    "                 eps_end= 0.1,\n",
    "                 eps_end_episode = 100,\n",
    "                 episodes=1000,\n",
    "                 gamma=0.95):\n",
    "        self.action_space = ['right', 'down', 'left', 'up']\n",
    "        self.env_name = env_name\n",
    "        self.env = self.create_env(env_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mode = 'rgb'\n",
    "        self.size = 32\n",
    "        \n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_end_episode = eps_end_episode        \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        num_actions = self.env.action_space.n\n",
    "        self.table = defaultdict(lambda: np.zeros(shape=(num_actions,)))\n",
    "        \n",
    "        self.episode_return_history = []\n",
    "        self.epsilon_history = []\n",
    "        \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(self.eps_end, \n",
    "                    ((self.eps_end - self.eps_start) / self.eps_end_episode)*episode \n",
    "                    + self.eps_start)\n",
    "        \n",
    "    def create_env(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        env = RewardWrapper(env)\n",
    "        env = CoordsObsWrapper(env)\n",
    "        env = FourDirectionsActionWrapper(env)\n",
    "        return env        \n",
    "\n",
    "    def e_greedy(self, state, epsilon):\n",
    "        if random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.table[state])\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def log_metrics(self, episode_return, epsilon):\n",
    "        self.episode_return_history.append(episode_return)\n",
    "        self.epsilon_history.append(epsilon)\n",
    "        \n",
    "    def plot(self):\n",
    "        clear_output(wait = True)   \n",
    "        plt.figure(figsize=(15,10)) \n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        ax3 = plt.subplot(2,1,2)        \n",
    "        plot_return(ax1, self.episode_return_history)\n",
    "        plot_epsilon(ax2, self.epsilon_history)\n",
    "        plot_q_value_func(ax3, self.table, self.env)        \n",
    "        plt.pause(0.000001)\n",
    "        \n",
    "    def demo_plot(self, state):\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(15,5), gridspec_kw={'width_ratios': [2, 1]}) \n",
    "        img = self.env.render(mode='rgb', highlight=False)\n",
    "        plot_q_value_func(ax1, self.table, self.env, alpha = 0.4, img = img)\n",
    "        plt.ylim(-0.1, 1)\n",
    "        \n",
    "        ax2.set_ylabel('Q-Values')\n",
    "        ax2.set_title('Q-Values')\n",
    "        sns.barplot(x=['right', 'down', 'left', 'up'], y=self.table[state], ax = ax2)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.5)\n",
    "\n",
    "    def demo(self, epsilon, random_start = True):\n",
    "        done = False\n",
    "        state = self.env.reset(random_start = True)\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            while not done:\n",
    "                self.demo_plot(state)\n",
    "                action = self.e_greedy(state,epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "            state = self.env.reset(random_start = True)\n",
    "            done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239461ae",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "Training a SARSA agent\n",
    "\n",
    "TODO: add some notes, pictures, equations of SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b93fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:12:05.771560Z",
     "start_time": "2021-09-29T07:12:05.765796Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentSarsa(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_sarsa(self, action, state, next_state, next_action, reward, done):\n",
    "        v_estimate = 0 if done else self.gamma*self.table[next_state][next_action]        \n",
    "        self.table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        action = self.e_greedy(state, self.get_epsilon(0))\n",
    "\n",
    "        episode_return = 0\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep)\n",
    "            while not done:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                if not done:\n",
    "                    next_action = self.e_greedy(next_state, epsilon)\n",
    "                self.update_sarsa(action, state, next_state,\n",
    "                                  next_action, reward, done)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            state = self.env.reset()\n",
    "            action = self.e_greedy(state, epsilon)\n",
    "            done = False\n",
    "            \n",
    "            self.log_metrics(episode_return, epsilon)\n",
    "            episode_return = 0\n",
    "            \n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde6ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:23:24.055111Z",
     "start_time": "2021-09-29T07:12:05.773670Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa = AgentSarsa(env_name='MiniGrid-DistShift1-v0', episodes=100) #\n",
    "agent_sarsa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca3ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.309684Z",
     "start_time": "2021-09-29T07:23:24.056397Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_sarsa.demo(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70356c",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Training a Q-learning agent\n",
    "\n",
    "//TODO:\n",
    "add some notes and theory about Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01964e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:24:05.328505Z",
     "start_time": "2021-09-29T07:24:05.312569Z"
    }
   },
   "outputs": [],
   "source": [
    "class AgentQL(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def update_q_table(self, action, state, next_state, reward, done):\n",
    "        v_estimate = 0 if done else self.gamma*np.max(self.table[next_state])\n",
    "        self.table[state][action] += self.learning_rate*(\n",
    "            reward + v_estimate - self.table[state][action])\n",
    "\n",
    "    def train(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        step = 0\n",
    "        episode_return = 0\n",
    "        for ep in range(self.episodes):\n",
    "            epsilon = self.get_epsilon(ep)\n",
    "            while not done:\n",
    "                action = self.e_greedy(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                self.update_q_table(action, state, next_state, reward, done)\n",
    "                state = next_state\n",
    "                step += 1\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            self.log_metrics(episode_return, epsilon)\n",
    "            episode_return = 0\n",
    "            \n",
    "            if ep % 10 == 0:\n",
    "                self.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeb547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.614861Z",
     "start_time": "2021-09-29T07:24:05.330277Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q = AgentQL(env_name='MiniGrid-DistShift1-v0')\n",
    "agent_q.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c7042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.618202Z",
     "start_time": "2021-09-29T07:34:51.615936Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_q.demo(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a80748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T07:34:51.725028Z",
     "start_time": "2021-09-29T07:34:51.619733Z"
    }
   },
   "outputs": [],
   "source": [
    "dis_df = pd.DataFrame({'SARSA': agent_sarsa.episode_return_history, 'Q-Learning': agent_q.episode_return_history})\n",
    "sns.displot(dis_df, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734e6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
